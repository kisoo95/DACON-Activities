{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final Draft in anomaly detection.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMBx5mvZfYGKli+EzL5MfIW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EEEoDfompF3b","executionInfo":{"status":"ok","timestamp":1660125951230,"user_tz":-540,"elapsed":3380,"user":{"displayName":"­신기수","userId":"10948147504762254674"}},"outputId":"0ec6c295-3433-495d-bd50-3ff6a7e303b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"]}],"source":["# 구글 드라이브 입력\n","from google.colab import drive\n","drive.mount('/content/gdrive/')"]},{"cell_type":"markdown","source":["설치 해야하는 것들 및 함수들"],"metadata":{"id":"W2nnEzGl3jh0"}},{"cell_type":"code","source":["# pip install 요소들 정리\n","# annoy는 반드시 visual studio build tools 설치\n","! pip install -q annoy\n","! pip install -q FRUFS\n","! pip install -q pacmap"],"metadata":{"id":"J98RvVnMq771","executionInfo":{"status":"ok","timestamp":1660125976011,"user_tz":-540,"elapsed":24789,"user":{"displayName":"­신기수","userId":"10948147504762254674"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# import할 요소들 정리\n","import copy\n","import pandas as pd\n","import numpy as np\n","import warnings\n","import pacmap\n","\n","from FRUFS import FRUFS\n","from lightgbm import LGBMRegressor\n","\n","from sklearn.ensemble import IsolationForest\n","\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import recall_score\n","\n","from sklearn.decomposition import PCA\n","from sklearn.decomposition import KernelPCA\n","\n","import matplotlib.pyplot as plt\n","\n","import os\n","\n","import seaborn as sns\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager\n","from sklearn import svm\n","\n","from scipy.stats import ranksums\n","\n","# 파일 위치 고정\n","os.chdir(\"/content/gdrive/MyDrive/creditcardfraud\")\n","\n","# Train dataset\n","train_df = pd.read_csv('train.csv')\n","train_df = train_df.iloc[:,1:]\n","\n","# Validation dataset\n","val_df = pd.read_csv('val.csv')\n","ori_val_df = val_df.iloc[:,1:]\n","val_class = val_df.iloc[:,31]\n","val_df = val_df.iloc[:,1:31]\n","\n","# Test dataset\n","test_df = pd.read_csv('test.csv')\n","test_df = test_df.iloc[:,1:]\n","\n","warnings.filterwarnings(action='ignore')"],"metadata":{"id":"7pULCAakpcuZ","executionInfo":{"status":"ok","timestamp":1660125986454,"user_tz":-540,"elapsed":10449,"user":{"displayName":"­신기수","userId":"10948147504762254674"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# IsolationForest 모델 출력 (1:정상, -1:불량(사기)) 이므로 (0:정상, 1:불량(사기))로 Label 변환\n","def get_pred_label(model_pred):    \n","    model_pred = np.where(model_pred == 1, 0, model_pred)\n","    model_pred = np.where(model_pred == -1, 1, model_pred)\n","    return model_pred\n","\n","# IsolationForest 예측만 하는 함수\n","def iso_for_model_prediction(the_contamination, trtrtr):\n","    # train dataset으로 isolationforest 모델 학습\n","    model_only_train = IsolationForest(n_estimators=1000, contamination=the_contamination, verbose=0)\n","    model_only_train.fit(trtrtr)\n","    \n","    # train dataset의 isolationforest 모델로 예측\n","    train_pred = model_only_train.predict(trtrtr)\n","    train_pred = get_pred_label(train_pred)\n","    \n","    return train_pred\n","\n","# Pacmac + IsolationForest 예측과 비교할 수 있는 함수\n","def pac_iso_for_model_comparing(the_contamination, trtrtr, low_dim, compare_class):\n","    # train dataset으로 isolationforest 모델 학습\n","    dlatl_embedding = pacmap.PaCMAP(n_components=low_dim, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, num_iters = 1000, verbose = True)\n","    pac_mac_train = dlatl_embedding.fit_transform(np.array(trtrtr), init=\"pca\")\n","    \n","    model_train_compare = IsolationForest(n_estimators=1000, contamination=the_contamination, verbose=0)\n","    model_train_compare.fit(pac_mac_train)\n","    \n","    # train dataset의 isolationforest 모델로 예측\n","    train_pred = model_train_compare.predict(pac_mac_train)\n","    train_pred = get_pred_label(train_pred)\n","    \n","    # train dataset의 예측치와 compare data의 수치 비교\n","    train_score = f1_score(compare_class, train_pred, average='macro')\n","\n","    print(f'Compared Macro F1 Score : [{train_score}]')\n","    print(classification_report(compare_class, train_score))\n","\n","# IQR Method에서 경계값을 나타낸 함수\n","def iqr_outlier(ddff):\n","    q1 = ddff.quantile(0.25)\n","    q3 = ddff.quantile(0.75)\n","\n","    iqr = q3 - q1\n","\n","    lower_bound = q1 - (1.5 * iqr) \n","    upper_bound = q3 + (1.5 * iqr)\n","\n","    return pd.concat([lower_bound, upper_bound], axis= 1).T"],"metadata":{"id":"fYaK5Zib1mii","executionInfo":{"status":"ok","timestamp":1660125986456,"user_tz":-540,"elapsed":13,"user":{"displayName":"­신기수","userId":"10948147504762254674"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Validation dataset의 통계정보를 이용한 1차로 변수 선택합니다. 테스트 데이터 중 랜덤 샘플된 것이므로 Validation dataset의 통계정보로 충분히 정보를 얻을 수 있습니다.아래의 기준은 outlier의 중위값이 IQR method의 범위 내에 있는지 확인합니다. 저는 outlier의 중위값이 적어도 IQR method의 범위 밖에 있어야 outlier 판단하기 쉽다고 생각하였습니다."],"metadata":{"id":"q6OjAXiT3q4L"}},{"cell_type":"code","source":["## 기본적인 변수 선택(1)\n","def first_variation_selection(dfdfdf):\n","    # Validation dataset의 outlier들의 중앙값이 Validation dataset의 IQR Method에서 경계값 사이에 있으면 이상치를 판단하지 못하는 변수라고 정했다.\n","    # class 열도 있으므로 1개 제외\n","    how_many_var = (len(dfdfdf.columns) - 1)\n","    new_var = []\n","\n","    for what_val in range(how_many_var):\n","        if (iqr_outlier(dfdfdf).iloc[0,what_val] < dfdfdf.iloc[np.where(dfdfdf['Class'] == 1)].quantile(0.5)[what_val] < iqr_outlier(dfdfdf).iloc[1,what_val]):\n","            continue\n","        else:\n","            new_var.append(what_val)\n","\n","    return new_var\n","\n","superior_var1 = first_variation_selection(ori_val_df)\n","print(superior_var1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xKld2opgreZg","executionInfo":{"status":"ok","timestamp":1660125990142,"user_tz":-540,"elapsed":3697,"user":{"displayName":"­신기수","userId":"10948147504762254674"}},"outputId":"99d2b550-bac9-4b48-943e-55e3ea8cac3f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 2, 3, 6, 8, 9, 10, 11, 13, 15, 16, 17]\n"]}]},{"cell_type":"markdown","source":["이번엔 2차 변수 선택입니다. 두 가지 변수 방법을 사용하여 결과물을 합집합화 하였습니다. 첫번째는 Wilcoxon rank-sum test를 이용해 Validation dataset의 outlier들의 중앙값과 Validation dataset의 inlier들의 중앙값의 차이를 검정해보고 p-value가 가장 낮은 5개 변수를 뽑았습니다. 두번째는 FRUFS의 LGBMRegressor을 이용하여 변수 중요도를 판단하였고 중요도가 가장 높은 5개 변수를 뽑았습니다."],"metadata":{"id":"Cq8Z3-P-4g1M"}},{"cell_type":"code","source":["## 기본적인 변수 선택(2)\n","# 2차 변수 선택\n","def second_variation_selection(dfdfdf, trtrtr, nnn_var):\n","    ranksum_pval = []\n","    for what_val in nnn_var:\n","        ranksum_pval.append(ranksums(dfdfdf.iloc[np.where(dfdfdf['Class'] == 1)].iloc[:,what_val], dfdfdf.iloc[np.where(dfdfdf['Class'] == 0)].iloc[:,what_val]).pvalue)\n","\n","    Wilcoxon_rank_sum_pval_var = list(pd.DataFrame({'pval':ranksum_pval, 'col':dfdfdf.columns[nnn_var]}).sort_values(by=['pval']).iloc[range(5),1])\n","\n","    print(Wilcoxon_rank_sum_pval_var)\n","\n","    # FRUFS를 이용하여 변수 선택\n","    # core가 많으면 n_jobs 조정을 하면 됨.\n","    model_frufs = FRUFS(model_r=LGBMRegressor(random_state=28), k=5, n_jobs=-1, verbose=1, random_state=28)\n","    df_train_pruned = model_frufs.fit_transform(trtrtr.iloc[:,nnn_var])\n","    FRUFS_LGBMRegressor_var = list(df_train_pruned.columns)\n","\n","    #plt.figure(figsize=(5, 6), dpi=100)\n","    #model_frufs.feature_importance()\n","    print(FRUFS_LGBMRegressor_var)\n","\n","    # 종합\n","    all_var_in_td = list(trtrtr.columns)\n","    new_var_set = list(set(Wilcoxon_rank_sum_pval_var + FRUFS_LGBMRegressor_var))\n","    new_var_list = []\n","\n","    for nvs in new_var_set:\n","        new_var_list.append(all_var_in_td.index(nvs))\n","\n","    new_var_list.sort()\n","    return new_var_list\n","\n","superior_var2 = second_variation_selection(ori_val_df, train_df, superior_var1)\n","print(superior_var2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aiCo9p4Ixcj6","executionInfo":{"status":"ok","timestamp":1660126051750,"user_tz":-540,"elapsed":61619,"user":{"displayName":"­신기수","userId":"10948147504762254674"}},"outputId":"7af88949-9fd6-48f1-a6f9-371e14617d3e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["['V10', 'V14', 'V11', 'V4', 'V12']\n"]},{"output_type":"stream","name":"stderr","text":["[Parallel(n_jobs=60)]: Using backend LokyBackend with 60 concurrent workers.\n","[Parallel(n_jobs=60)]: Done  10 out of  12 | elapsed:  1.0min remaining:   12.2s\n"]},{"output_type":"stream","name":"stdout","text":["['V10', 'V4', 'V14', 'V17', 'V16']\n","[3, 9, 10, 11, 13, 15, 16]\n"]},{"output_type":"stream","name":"stderr","text":["[Parallel(n_jobs=60)]: Done  12 out of  12 | elapsed:  1.0min finished\n"]}]},{"cell_type":"code","source":["## 기본적인 변수 선택(3)\n","# 선택한 변수 제외 나머지 변수 모임\n","inferior_var2 = [x for x in range(30) if x not in superior_var2]\n","print(inferior_var2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gqCSnpjZCUs7","executionInfo":{"status":"ok","timestamp":1660126051751,"user_tz":-540,"elapsed":34,"user":{"displayName":"­신기수","userId":"10948147504762254674"}},"outputId":"51e7ebc2-1188-4667-dff1-b303d65327c9"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 1, 2, 4, 5, 6, 7, 8, 12, 14, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n"]}]},{"cell_type":"markdown","source":["Baseline에 보시면 아시겠지만 validation set의 사기 거래 비율을 탐색하였는데 저는 이보다 조금 높은 수치를 사용하였습니다."],"metadata":{"id":"4Sncow6C473S"}},{"cell_type":"code","source":["## validation set의 사기 거래 비율 탐색\n","ori_val_normal, ori_val_fraud = ori_val_df['Class'].value_counts()\n","ori_val_contamination = ori_val_fraud / ori_val_normal\n","print(f'Validation contamination:[{ori_val_contamination}]')\n","# 이대로 하지 않고 조정을 함"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t2s3Iquh2gbo","executionInfo":{"status":"ok","timestamp":1660126051753,"user_tz":-540,"elapsed":30,"user":{"displayName":"­신기수","userId":"10948147504762254674"}},"outputId":"fc183749-e131-4c42-ee6a-d731bde6492e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation contamination:[0.0010551491277433877]\n"]}]},{"cell_type":"markdown","source":["본격적으로 pacmap과 isolation forest를 이용하여 예측하고자 합니다. pacmap은 차원축소를 하는 과정에 있어 랜덤하게 이동합니다. isolation forest도 랜덤하게 변수를 선택하기 때문에 결과가 불분명합니다. 따라서 저는 hhmm번 진행하고 voting을 통해 결과를 도출하였습니다."],"metadata":{"id":"R2T8mO5X5DN9"}},{"cell_type":"code","source":["## pacmap과 isolation forest 1차 이용(1)\n","# pacmap과 isolation forest를 이용한 1차 예측\n","hhmm = 3\n","what_val = superior_var2\n","for num in range(hhmm):\n","    embedding_1 = pacmap.PaCMAP(n_components=len(what_val), n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, num_iters = 1000, verbose = True)\n","    pacmac_train_1 = embedding_1.fit_transform(np.array(train_df.iloc[:,what_val]), init=\"pca\")\n","    pacmac_val_1 = embedding_1.transform(np.array(val_df.iloc[:,what_val]), basis=np.array(train_df.iloc[:,what_val]))\n","    pacmac_test_1 = embedding_1.transform(np.array(test_df.iloc[:,what_val]), basis=np.array(train_df.iloc[:,what_val]))\n","\n","    pac_model_1 = IsolationForest(n_estimators=1000, contamination=0.00121, verbose=0)\n","    pac_model_1.fit(pacmac_train_1[:,[1,2,3]]) \n","\n","    if num == 0:\n","        train_pred_set_1 = pac_model_1.predict(pacmac_train_1[:,[1,2,3]]) # model prediction\n","        train_pred_set_1 = get_pred_label(train_pred_set_1)\n","\n","        val_pred_set_1 = pac_model_1.predict(pacmac_val_1[:,[1,2,3]]) # model prediction\n","        val_pred_set_1 = get_pred_label(val_pred_set_1)\n","\n","        test_pred_set_1 = pac_model_1.predict(pacmac_test_1[:,[1,2,3]]) # model prediction\n","        test_pred_set_1 = get_pred_label(test_pred_set_1)\n","    else:\n","        train_pred_1 = pac_model_1.predict(pacmac_train_1[:,[1,2,3]]) # model prediction\n","        train_pred_1 = get_pred_label(train_pred_1)\n","        train_pred_set_1 = train_pred_set_1 + train_pred_1\n","\n","        val_pred_1 = pac_model_1.predict(pacmac_val_1[:,[1,2,3]]) # model prediction\n","        val_pred_1 = get_pred_label(val_pred_1)\n","        val_pred_set_1 = val_pred_set_1 + val_pred_1\n","\n","        test_pred_1 = pac_model_1.predict(pacmac_test_1[:,[1,2,3]]) # model prediction\n","        test_pred_1 = get_pred_label(test_pred_1)\n","        test_pred_set_1 = test_pred_set_1 + test_pred_1\n","\n","train_pred_set_1 = train_pred_set_1/hhmm\n","val_pred_set_1 = val_pred_set_1/hhmm\n","test_pred_set_1 = test_pred_set_1/hhmm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Muij_O8F7xE1","executionInfo":{"status":"ok","timestamp":1660109947102,"user_tz":-540,"elapsed":5316577,"user":{"displayName":"­신기수","userId":"10948147504762254674"}},"outputId":"f020ad7c-3154-479e-dc04-12be74c5aa3b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["X is normalized\n","PaCMAP(n_neighbors=26, n_MN=13, n_FP=52, distance=euclidean, lr=1.0, n_iters=1000, apply_pca=True, opt_method='adam', verbose=True, intermediate=False, seed=None)\n","Finding pairs\n","Found nearest neighbor\n","Calculated sigma\n","Found scaled dist\n","Pairs sampled successfully.\n","((2959892, 2), (1479946, 2), (5919784, 2))\n","Initial Loss: 3658542.0\n","Iteration:   10, Loss: 2403497.750000\n","Iteration:   20, Loss: 2149115.500000\n","Iteration:   30, Loss: 2015282.625000\n","Iteration:   40, Loss: 1911173.750000\n","Iteration:   50, Loss: 1811854.250000\n","Iteration:   60, Loss: 1706509.750000\n","Iteration:   70, Loss: 1588218.500000\n","Iteration:   80, Loss: 1448030.250000\n","Iteration:   90, Loss: 1271641.875000\n","Iteration:  100, Loss: 1005694.500000\n","Iteration:  110, Loss: 1315544.375000\n","Iteration:  120, Loss: 1293438.500000\n","Iteration:  130, Loss: 1283935.000000\n","Iteration:  140, Loss: 1280798.875000\n","Iteration:  150, Loss: 1280236.500000\n","Iteration:  160, Loss: 1280233.000000\n","Iteration:  170, Loss: 1280348.375000\n","Iteration:  180, Loss: 1281080.375000\n","Iteration:  190, Loss: 1281946.750000\n","Iteration:  200, Loss: 1282200.625000\n","Iteration:  210, Loss: 540239.187500\n","Iteration:  220, Loss: 535146.625000\n","Iteration:  230, Loss: 530556.312500\n","Iteration:  240, Loss: 528175.937500\n","Iteration:  250, Loss: 526238.000000\n","Iteration:  260, Loss: 524554.562500\n","Iteration:  270, Loss: 523185.531250\n","Iteration:  280, Loss: 522184.750000\n","Iteration:  290, Loss: 521286.312500\n","Iteration:  300, Loss: 520333.156250\n","Iteration:  310, Loss: 519354.593750\n","Iteration:  320, Loss: 518382.000000\n","Iteration:  330, Loss: 517498.125000\n","Iteration:  340, Loss: 516748.218750\n","Iteration:  350, Loss: 516152.687500\n","Iteration:  360, Loss: 515710.718750\n","Iteration:  370, Loss: 515374.562500\n","Iteration:  380, Loss: 515137.968750\n","Iteration:  390, Loss: 514963.375000\n","Iteration:  400, Loss: 514816.843750\n","Iteration:  410, Loss: 514664.937500\n","Iteration:  420, Loss: 514502.593750\n","Iteration:  430, Loss: 514299.812500\n","Iteration:  440, Loss: 514077.000000\n","Iteration:  450, Loss: 513857.093750\n","Iteration:  460, Loss: 513639.531250\n","Iteration:  470, Loss: 513442.343750\n","Iteration:  480, Loss: 513269.843750\n","Iteration:  490, Loss: 513109.500000\n","Iteration:  500, Loss: 512949.093750\n","Iteration:  510, Loss: 512782.687500\n","Iteration:  520, Loss: 512619.062500\n","Iteration:  530, Loss: 512464.000000\n","Iteration:  540, Loss: 512318.812500\n","Iteration:  550, Loss: 512191.812500\n","Iteration:  560, Loss: 512072.250000\n","Iteration:  570, Loss: 511962.781250\n","Iteration:  580, Loss: 511869.687500\n","Iteration:  590, Loss: 511792.125000\n","Iteration:  600, Loss: 511727.875000\n","Iteration:  610, Loss: 511676.843750\n","Iteration:  620, Loss: 511635.093750\n","Iteration:  630, Loss: 511604.718750\n","Iteration:  640, Loss: 511588.250000\n","Iteration:  650, Loss: 511576.437500\n","Iteration:  660, Loss: 511568.906250\n","Iteration:  670, Loss: 511563.656250\n","Iteration:  680, Loss: 511560.000000\n","Iteration:  690, Loss: 511563.437500\n","Iteration:  700, Loss: 511570.187500\n","Iteration:  710, Loss: 511572.937500\n","Iteration:  720, Loss: 511579.625000\n","Iteration:  730, Loss: 511592.000000\n","Iteration:  740, Loss: 511604.875000\n","Iteration:  750, Loss: 511618.281250\n","Iteration:  760, Loss: 511631.187500\n","Iteration:  770, Loss: 511643.531250\n","Iteration:  780, Loss: 511645.656250\n","Iteration:  790, Loss: 511641.656250\n","Iteration:  800, Loss: 511636.062500\n","Iteration:  810, Loss: 511629.875000\n","Iteration:  820, Loss: 511623.875000\n","Iteration:  830, Loss: 511619.750000\n","Iteration:  840, Loss: 511615.531250\n","Iteration:  850, Loss: 511613.718750\n","Iteration:  860, Loss: 511608.781250\n","Iteration:  870, Loss: 511601.531250\n","Iteration:  880, Loss: 511593.031250\n","Iteration:  890, Loss: 511581.625000\n","Iteration:  900, Loss: 511566.562500\n","Iteration:  910, Loss: 511548.343750\n","Iteration:  920, Loss: 511526.906250\n","Iteration:  930, Loss: 511502.000000\n","Iteration:  940, Loss: 511475.125000\n","Iteration:  950, Loss: 511446.656250\n","Iteration:  960, Loss: 511416.468750\n","Iteration:  970, Loss: 511385.437500\n","Iteration:  980, Loss: 511354.031250\n","Iteration:  990, Loss: 511322.187500\n","Iteration: 1000, Loss: 511291.843750\n","Elapsed time: 1287.07s\n","X is normalized.\n","X is normalized.\n","Found nearest neighbor\n","Found scaled dist\n","(740012, 2)\n","Initial Loss: 1395646.125\n","Iteration:   10, Loss: 701953.625000\n","Iteration:   20, Loss: 356882.718750\n","Iteration:   30, Loss: 259094.468750\n","Iteration:   40, Loss: 222042.156250\n","Iteration:   50, Loss: 209354.250000\n","Iteration:   60, Loss: 206091.078125\n","Iteration:   70, Loss: 204527.250000\n","Iteration:   80, Loss: 204111.296875\n","Iteration:   90, Loss: 203872.609375\n","Iteration:  100, Loss: 203834.765625\n","Iteration:  110, Loss: 306783.500000\n","Iteration:  120, Loss: 306223.718750\n","Iteration:  130, Loss: 306065.531250\n","Iteration:  140, Loss: 306022.125000\n","Iteration:  150, Loss: 306011.093750\n","Iteration:  160, Loss: 305992.843750\n","Iteration:  170, Loss: 305979.718750\n","Iteration:  180, Loss: 305979.437500\n","Iteration:  190, Loss: 305990.375000\n","Iteration:  200, Loss: 306004.593750\n","Iteration:  210, Loss: 101976.718750\n","Iteration:  220, Loss: 101916.632812\n","Iteration:  230, Loss: 101903.851562\n","Iteration:  240, Loss: 101898.085938\n","Iteration:  250, Loss: 101896.109375\n","Iteration:  260, Loss: 101896.101562\n","Iteration:  270, Loss: 101896.476562\n","Iteration:  280, Loss: 101896.562500\n","Iteration:  290, Loss: 101896.523438\n","Iteration:  300, Loss: 101896.445312\n","Iteration:  310, Loss: 101896.468750\n","Iteration:  320, Loss: 101896.476562\n","Iteration:  330, Loss: 101896.484375\n","Iteration:  340, Loss: 101896.429688\n","Iteration:  350, Loss: 101896.476562\n","Iteration:  360, Loss: 101896.437500\n","Iteration:  370, Loss: 101896.437500\n","Iteration:  380, Loss: 101896.468750\n","Iteration:  390, Loss: 101896.453125\n","Iteration:  400, Loss: 101896.445312\n","Iteration:  410, Loss: 101896.460938\n","Iteration:  420, Loss: 101896.453125\n","Iteration:  430, Loss: 101896.453125\n","Iteration:  440, Loss: 101896.453125\n","Iteration:  450, Loss: 101896.453125\n","Iteration:  460, Loss: 101896.445312\n","Iteration:  470, Loss: 101896.460938\n","Iteration:  480, Loss: 101896.453125\n","Iteration:  490, Loss: 101896.460938\n","Iteration:  500, Loss: 101896.453125\n","Iteration:  510, Loss: 101896.460938\n","Iteration:  520, Loss: 101896.437500\n","Iteration:  530, Loss: 101896.453125\n","Iteration:  540, Loss: 101896.445312\n","Iteration:  550, Loss: 101896.453125\n","Iteration:  560, Loss: 101896.453125\n","Iteration:  570, Loss: 101896.453125\n","Iteration:  580, Loss: 101896.445312\n","Iteration:  590, Loss: 101896.460938\n","Iteration:  600, Loss: 101896.453125\n","Iteration:  610, Loss: 101896.460938\n","Iteration:  620, Loss: 101896.437500\n","Iteration:  630, Loss: 101896.445312\n","Iteration:  640, Loss: 101896.453125\n","Iteration:  650, Loss: 101896.460938\n","Iteration:  660, Loss: 101896.468750\n","Iteration:  670, Loss: 101896.437500\n","Iteration:  680, Loss: 101896.453125\n","Iteration:  690, Loss: 101896.453125\n","Iteration:  700, Loss: 101896.460938\n","Iteration:  710, Loss: 101896.445312\n","Iteration:  720, Loss: 101896.453125\n","Iteration:  730, Loss: 101896.453125\n","Iteration:  740, Loss: 101896.445312\n","Iteration:  750, Loss: 101896.445312\n","Iteration:  760, Loss: 101896.460938\n","Iteration:  770, Loss: 101896.453125\n","Iteration:  780, Loss: 101896.460938\n","Iteration:  790, Loss: 101896.453125\n","Iteration:  800, Loss: 101896.453125\n","Iteration:  810, Loss: 101896.460938\n","Iteration:  820, Loss: 101896.437500\n","Iteration:  830, Loss: 101896.445312\n","Iteration:  840, Loss: 101896.445312\n","Iteration:  850, Loss: 101896.460938\n","Iteration:  860, Loss: 101896.453125\n","Iteration:  870, Loss: 101896.460938\n","Iteration:  880, Loss: 101896.445312\n","Iteration:  890, Loss: 101896.453125\n","Iteration:  900, Loss: 101896.460938\n","Iteration:  910, Loss: 101896.445312\n","Iteration:  920, Loss: 101896.453125\n","Iteration:  930, Loss: 101896.445312\n","Iteration:  940, Loss: 101896.468750\n","Iteration:  950, Loss: 101896.437500\n","Iteration:  960, Loss: 101896.445312\n","Iteration:  970, Loss: 101896.445312\n","Iteration:  980, Loss: 101896.445312\n","Iteration:  990, Loss: 101896.437500\n","Iteration: 1000, Loss: 101896.453125\n","Elapsed time: 0:01:06.073969\n","X is normalized.\n","X is normalized.\n","Found nearest neighbor\n","Found scaled dist\n","(3705078, 2)\n","Initial Loss: 7100975.5\n","Iteration:   10, Loss: 3482230.500000\n","Iteration:   20, Loss: 1787767.125000\n","Iteration:   30, Loss: 1302890.375000\n","Iteration:   40, Loss: 1108012.000000\n","Iteration:   50, Loss: 1040582.250000\n","Iteration:   60, Loss: 1026960.625000\n","Iteration:   70, Loss: 1021017.625000\n","Iteration:   80, Loss: 1019524.000000\n","Iteration:   90, Loss: 1018781.687500\n","Iteration:  100, Loss: 1018666.875000\n","Iteration:  110, Loss: 1517982.250000\n","Iteration:  120, Loss: 1514081.250000\n","Iteration:  130, Loss: 1513246.750000\n","Iteration:  140, Loss: 1513142.375000\n","Iteration:  150, Loss: 1513072.750000\n","Iteration:  160, Loss: 1512953.125000\n","Iteration:  170, Loss: 1512891.500000\n","Iteration:  180, Loss: 1512905.000000\n","Iteration:  190, Loss: 1513001.125000\n","Iteration:  200, Loss: 1513142.125000\n","Iteration:  210, Loss: 509543.375000\n","Iteration:  220, Loss: 509317.812500\n","Iteration:  230, Loss: 509279.468750\n","Iteration:  240, Loss: 509263.093750\n","Iteration:  250, Loss: 509257.843750\n","Iteration:  260, Loss: 509255.562500\n","Iteration:  270, Loss: 509255.656250\n","Iteration:  280, Loss: 509255.187500\n","Iteration:  290, Loss: 509254.593750\n","Iteration:  300, Loss: 509254.812500\n","Iteration:  310, Loss: 509254.593750\n","Iteration:  320, Loss: 509254.375000\n","Iteration:  330, Loss: 509254.218750\n","Iteration:  340, Loss: 509254.312500\n","Iteration:  350, Loss: 509254.062500\n","Iteration:  360, Loss: 509254.000000\n","Iteration:  370, Loss: 509254.000000\n","Iteration:  380, Loss: 509254.062500\n","Iteration:  390, Loss: 509254.156250\n","Iteration:  400, Loss: 509254.031250\n","Iteration:  410, Loss: 509254.062500\n","Iteration:  420, Loss: 509254.093750\n","Iteration:  430, Loss: 509254.093750\n","Iteration:  440, Loss: 509254.062500\n","Iteration:  450, Loss: 509254.093750\n","Iteration:  460, Loss: 509254.093750\n","Iteration:  470, Loss: 509254.093750\n","Iteration:  480, Loss: 509254.062500\n","Iteration:  490, Loss: 509254.093750\n","Iteration:  500, Loss: 509254.062500\n","Iteration:  510, Loss: 509254.093750\n","Iteration:  520, Loss: 509254.093750\n","Iteration:  530, Loss: 509254.093750\n","Iteration:  540, Loss: 509254.062500\n","Iteration:  550, Loss: 509254.062500\n","Iteration:  560, Loss: 509254.093750\n","Iteration:  570, Loss: 509254.062500\n","Iteration:  580, Loss: 509254.093750\n","Iteration:  590, Loss: 509254.093750\n","Iteration:  600, Loss: 509254.062500\n","Iteration:  610, Loss: 509254.093750\n","Iteration:  620, Loss: 509254.062500\n","Iteration:  630, Loss: 509254.062500\n","Iteration:  640, Loss: 509254.125000\n","Iteration:  650, Loss: 509254.125000\n","Iteration:  660, Loss: 509254.093750\n","Iteration:  670, Loss: 509254.062500\n","Iteration:  680, Loss: 509254.093750\n","Iteration:  690, Loss: 509254.125000\n","Iteration:  700, Loss: 509254.093750\n","Iteration:  710, Loss: 509254.062500\n","Iteration:  720, Loss: 509254.125000\n","Iteration:  730, Loss: 509254.093750\n","Iteration:  740, Loss: 509254.062500\n","Iteration:  750, Loss: 509254.093750\n","Iteration:  760, Loss: 509254.125000\n","Iteration:  770, Loss: 509254.093750\n","Iteration:  780, Loss: 509254.062500\n","Iteration:  790, Loss: 509254.062500\n","Iteration:  800, Loss: 509254.062500\n","Iteration:  810, Loss: 509254.093750\n","Iteration:  820, Loss: 509254.093750\n","Iteration:  830, Loss: 509254.125000\n","Iteration:  840, Loss: 509254.093750\n","Iteration:  850, Loss: 509254.125000\n","Iteration:  860, Loss: 509254.062500\n","Iteration:  870, Loss: 509254.062500\n","Iteration:  880, Loss: 509254.031250\n","Iteration:  890, Loss: 509254.093750\n","Iteration:  900, Loss: 509254.062500\n","Iteration:  910, Loss: 509254.062500\n","Iteration:  920, Loss: 509254.062500\n","Iteration:  930, Loss: 509254.062500\n","Iteration:  940, Loss: 509254.093750\n","Iteration:  950, Loss: 509254.000000\n","Iteration:  960, Loss: 509254.031250\n","Iteration:  970, Loss: 509254.062500\n","Iteration:  980, Loss: 509254.062500\n","Iteration:  990, Loss: 509254.062500\n","Iteration: 1000, Loss: 509254.093750\n","Elapsed time: 0:04:10.568326\n","X is normalized\n","PaCMAP(n_neighbors=26, n_MN=13, n_FP=52, distance=euclidean, lr=1.0, n_iters=1000, apply_pca=True, opt_method='adam', verbose=True, intermediate=False, seed=None)\n","Finding pairs\n","Found nearest neighbor\n","Calculated sigma\n","Found scaled dist\n","Pairs sampled successfully.\n","((2959892, 2), (1479946, 2), (5919784, 2))\n","Initial Loss: 3658542.0\n","Iteration:   10, Loss: 2402139.500000\n","Iteration:   20, Loss: 2148826.000000\n","Iteration:   30, Loss: 2015262.375000\n","Iteration:   40, Loss: 1911091.750000\n","Iteration:   50, Loss: 1811741.750000\n","Iteration:   60, Loss: 1706480.125000\n","Iteration:   70, Loss: 1588282.625000\n","Iteration:   80, Loss: 1448089.875000\n","Iteration:   90, Loss: 1271608.500000\n","Iteration:  100, Loss: 1005509.875000\n","Iteration:  110, Loss: 1315554.250000\n","Iteration:  120, Loss: 1293288.750000\n","Iteration:  130, Loss: 1283792.500000\n","Iteration:  140, Loss: 1280758.375000\n","Iteration:  150, Loss: 1280199.125000\n","Iteration:  160, Loss: 1280241.875000\n","Iteration:  170, Loss: 1280346.875000\n","Iteration:  180, Loss: 1281012.750000\n","Iteration:  190, Loss: 1281870.500000\n","Iteration:  200, Loss: 1282158.750000\n","Iteration:  210, Loss: 540256.625000\n","Iteration:  220, Loss: 535151.437500\n","Iteration:  230, Loss: 530552.750000\n","Iteration:  240, Loss: 528165.875000\n","Iteration:  250, Loss: 526232.562500\n","Iteration:  260, Loss: 524544.625000\n","Iteration:  270, Loss: 523174.750000\n","Iteration:  280, Loss: 522177.468750\n","Iteration:  290, Loss: 521245.375000\n","Iteration:  300, Loss: 520289.000000\n","Iteration:  310, Loss: 519311.656250\n","Iteration:  320, Loss: 518338.875000\n","Iteration:  330, Loss: 517462.750000\n","Iteration:  340, Loss: 516711.500000\n","Iteration:  350, Loss: 516122.000000\n","Iteration:  360, Loss: 515680.062500\n","Iteration:  370, Loss: 515353.875000\n","Iteration:  380, Loss: 515131.343750\n","Iteration:  390, Loss: 514959.843750\n","Iteration:  400, Loss: 514815.125000\n","Iteration:  410, Loss: 514666.656250\n","Iteration:  420, Loss: 514491.781250\n","Iteration:  430, Loss: 514293.343750\n","Iteration:  440, Loss: 514069.875000\n","Iteration:  450, Loss: 513847.187500\n","Iteration:  460, Loss: 513627.281250\n","Iteration:  470, Loss: 513429.031250\n","Iteration:  480, Loss: 513253.250000\n","Iteration:  490, Loss: 513091.875000\n","Iteration:  500, Loss: 512930.343750\n","Iteration:  510, Loss: 512760.968750\n","Iteration:  520, Loss: 512592.843750\n","Iteration:  530, Loss: 512437.343750\n","Iteration:  540, Loss: 512293.875000\n","Iteration:  550, Loss: 512163.625000\n","Iteration:  560, Loss: 512044.062500\n","Iteration:  570, Loss: 511934.437500\n","Iteration:  580, Loss: 511841.750000\n","Iteration:  590, Loss: 511763.468750\n","Iteration:  600, Loss: 511697.781250\n","Iteration:  610, Loss: 511640.000000\n","Iteration:  620, Loss: 511597.250000\n","Iteration:  630, Loss: 511565.125000\n","Iteration:  640, Loss: 511549.156250\n","Iteration:  650, Loss: 511539.343750\n","Iteration:  660, Loss: 511530.281250\n","Iteration:  670, Loss: 511524.875000\n","Iteration:  680, Loss: 511522.687500\n","Iteration:  690, Loss: 511526.625000\n","Iteration:  700, Loss: 511531.187500\n","Iteration:  710, Loss: 511537.343750\n","Iteration:  720, Loss: 511543.906250\n","Iteration:  730, Loss: 511555.312500\n","Iteration:  740, Loss: 511570.437500\n","Iteration:  750, Loss: 511585.843750\n","Iteration:  760, Loss: 511595.937500\n","Iteration:  770, Loss: 511609.375000\n","Iteration:  780, Loss: 511612.562500\n","Iteration:  790, Loss: 511610.000000\n","Iteration:  800, Loss: 511605.375000\n","Iteration:  810, Loss: 511600.718750\n","Iteration:  820, Loss: 511595.562500\n","Iteration:  830, Loss: 511591.343750\n","Iteration:  840, Loss: 511589.312500\n","Iteration:  850, Loss: 511584.312500\n","Iteration:  860, Loss: 511578.093750\n","Iteration:  870, Loss: 511568.812500\n","Iteration:  880, Loss: 511560.437500\n","Iteration:  890, Loss: 511548.750000\n","Iteration:  900, Loss: 511534.906250\n","Iteration:  910, Loss: 511517.781250\n","Iteration:  920, Loss: 511496.718750\n","Iteration:  930, Loss: 511471.968750\n","Iteration:  940, Loss: 511443.937500\n","Iteration:  950, Loss: 511413.500000\n","Iteration:  960, Loss: 511384.000000\n","Iteration:  970, Loss: 511352.531250\n","Iteration:  980, Loss: 511320.875000\n","Iteration:  990, Loss: 511288.625000\n","Iteration: 1000, Loss: 511258.187500\n","Elapsed time: 1204.22s\n","X is normalized.\n","X is normalized.\n","Found nearest neighbor\n","Found scaled dist\n","(740012, 2)\n","Initial Loss: 1395941.0\n","Iteration:   10, Loss: 701806.500000\n","Iteration:   20, Loss: 356428.218750\n","Iteration:   30, Loss: 258362.109375\n","Iteration:   40, Loss: 221787.921875\n","Iteration:   50, Loss: 209533.593750\n","Iteration:   60, Loss: 205978.406250\n","Iteration:   70, Loss: 204490.484375\n","Iteration:   80, Loss: 204043.484375\n","Iteration:   90, Loss: 203819.203125\n","Iteration:  100, Loss: 203776.281250\n","Iteration:  110, Loss: 306734.625000\n","Iteration:  120, Loss: 306143.687500\n","Iteration:  130, Loss: 305993.625000\n","Iteration:  140, Loss: 305962.843750\n","Iteration:  150, Loss: 305936.968750\n","Iteration:  160, Loss: 305915.250000\n","Iteration:  170, Loss: 305895.593750\n","Iteration:  180, Loss: 305892.562500\n","Iteration:  190, Loss: 305901.843750\n","Iteration:  200, Loss: 305920.718750\n","Iteration:  210, Loss: 101948.062500\n","Iteration:  220, Loss: 101887.687500\n","Iteration:  230, Loss: 101874.710938\n","Iteration:  240, Loss: 101869.296875\n","Iteration:  250, Loss: 101867.750000\n","Iteration:  260, Loss: 101867.750000\n","Iteration:  270, Loss: 101867.359375\n","Iteration:  280, Loss: 101867.195312\n","Iteration:  290, Loss: 101867.312500\n","Iteration:  300, Loss: 101867.343750\n","Iteration:  310, Loss: 101867.335938\n","Iteration:  320, Loss: 101867.218750\n","Iteration:  330, Loss: 101867.281250\n","Iteration:  340, Loss: 101867.296875\n","Iteration:  350, Loss: 101867.250000\n","Iteration:  360, Loss: 101867.242188\n","Iteration:  370, Loss: 101867.242188\n","Iteration:  380, Loss: 101867.218750\n","Iteration:  390, Loss: 101867.234375\n","Iteration:  400, Loss: 101867.250000\n","Iteration:  410, Loss: 101867.250000\n","Iteration:  420, Loss: 101867.250000\n","Iteration:  430, Loss: 101867.257812\n","Iteration:  440, Loss: 101867.250000\n","Iteration:  450, Loss: 101867.250000\n","Iteration:  460, Loss: 101867.250000\n","Iteration:  470, Loss: 101867.250000\n","Iteration:  480, Loss: 101867.250000\n","Iteration:  490, Loss: 101867.250000\n","Iteration:  500, Loss: 101867.250000\n","Iteration:  510, Loss: 101867.250000\n","Iteration:  520, Loss: 101867.250000\n","Iteration:  530, Loss: 101867.250000\n","Iteration:  540, Loss: 101867.250000\n","Iteration:  550, Loss: 101867.242188\n","Iteration:  560, Loss: 101867.250000\n","Iteration:  570, Loss: 101867.250000\n","Iteration:  580, Loss: 101867.242188\n","Iteration:  590, Loss: 101867.250000\n","Iteration:  600, Loss: 101867.250000\n","Iteration:  610, Loss: 101867.250000\n","Iteration:  620, Loss: 101867.250000\n","Iteration:  630, Loss: 101867.250000\n","Iteration:  640, Loss: 101867.242188\n","Iteration:  650, Loss: 101867.250000\n","Iteration:  660, Loss: 101867.242188\n","Iteration:  670, Loss: 101867.250000\n","Iteration:  680, Loss: 101867.250000\n","Iteration:  690, Loss: 101867.250000\n","Iteration:  700, Loss: 101867.250000\n","Iteration:  710, Loss: 101867.242188\n","Iteration:  720, Loss: 101867.242188\n","Iteration:  730, Loss: 101867.234375\n","Iteration:  740, Loss: 101867.250000\n","Iteration:  750, Loss: 101867.242188\n","Iteration:  760, Loss: 101867.242188\n","Iteration:  770, Loss: 101867.250000\n","Iteration:  780, Loss: 101867.242188\n","Iteration:  790, Loss: 101867.250000\n","Iteration:  800, Loss: 101867.250000\n","Iteration:  810, Loss: 101867.250000\n","Iteration:  820, Loss: 101867.242188\n","Iteration:  830, Loss: 101867.242188\n","Iteration:  840, Loss: 101867.250000\n","Iteration:  850, Loss: 101867.250000\n","Iteration:  860, Loss: 101867.250000\n","Iteration:  870, Loss: 101867.242188\n","Iteration:  880, Loss: 101867.257812\n","Iteration:  890, Loss: 101867.257812\n","Iteration:  900, Loss: 101867.242188\n","Iteration:  910, Loss: 101867.242188\n","Iteration:  920, Loss: 101867.250000\n","Iteration:  930, Loss: 101867.250000\n","Iteration:  940, Loss: 101867.250000\n","Iteration:  950, Loss: 101867.250000\n","Iteration:  960, Loss: 101867.250000\n","Iteration:  970, Loss: 101867.250000\n","Iteration:  980, Loss: 101867.250000\n","Iteration:  990, Loss: 101867.250000\n","Iteration: 1000, Loss: 101867.250000\n","Elapsed time: 0:01:02.468287\n","X is normalized.\n","X is normalized.\n","Found nearest neighbor\n","Found scaled dist\n","(3705078, 2)\n","Initial Loss: 7101768.0\n","Iteration:   10, Loss: 3478011.750000\n","Iteration:   20, Loss: 1782986.625000\n","Iteration:   30, Loss: 1299006.125000\n","Iteration:   40, Loss: 1106568.500000\n","Iteration:   50, Loss: 1041106.562500\n","Iteration:   60, Loss: 1026320.312500\n","Iteration:   70, Loss: 1020662.812500\n","Iteration:   80, Loss: 1019013.125000\n","Iteration:   90, Loss: 1018320.062500\n","Iteration:  100, Loss: 1018175.562500\n","Iteration:  110, Loss: 1517774.000000\n","Iteration:  120, Loss: 1513554.125000\n","Iteration:  130, Loss: 1512670.625000\n","Iteration:  140, Loss: 1512560.250000\n","Iteration:  150, Loss: 1512537.875000\n","Iteration:  160, Loss: 1512411.000000\n","Iteration:  170, Loss: 1512258.750000\n","Iteration:  180, Loss: 1512290.875000\n","Iteration:  190, Loss: 1512300.625000\n","Iteration:  200, Loss: 1512455.875000\n","Iteration:  210, Loss: 509312.968750\n","Iteration:  220, Loss: 509094.187500\n","Iteration:  230, Loss: 509047.781250\n","Iteration:  240, Loss: 509029.562500\n","Iteration:  250, Loss: 509021.875000\n","Iteration:  260, Loss: 509018.468750\n","Iteration:  270, Loss: 509016.750000\n","Iteration:  280, Loss: 509015.687500\n","Iteration:  290, Loss: 509015.593750\n","Iteration:  300, Loss: 509015.968750\n","Iteration:  310, Loss: 509015.281250\n","Iteration:  320, Loss: 509015.281250\n","Iteration:  330, Loss: 509015.250000\n","Iteration:  340, Loss: 509015.343750\n","Iteration:  350, Loss: 509015.031250\n","Iteration:  360, Loss: 509014.750000\n","Iteration:  370, Loss: 509015.000000\n","Iteration:  380, Loss: 509015.156250\n","Iteration:  390, Loss: 509015.125000\n","Iteration:  400, Loss: 509015.093750\n","Iteration:  410, Loss: 509015.062500\n","Iteration:  420, Loss: 509015.125000\n","Iteration:  430, Loss: 509015.031250\n","Iteration:  440, Loss: 509015.062500\n","Iteration:  450, Loss: 509015.062500\n","Iteration:  460, Loss: 509015.062500\n","Iteration:  470, Loss: 509015.062500\n","Iteration:  480, Loss: 509015.062500\n","Iteration:  490, Loss: 509015.031250\n","Iteration:  500, Loss: 509015.062500\n","Iteration:  510, Loss: 509015.093750\n","Iteration:  520, Loss: 509015.093750\n","Iteration:  530, Loss: 509015.062500\n","Iteration:  540, Loss: 509015.062500\n","Iteration:  550, Loss: 509015.031250\n","Iteration:  560, Loss: 509015.031250\n","Iteration:  570, Loss: 509015.093750\n","Iteration:  580, Loss: 509015.000000\n","Iteration:  590, Loss: 509015.062500\n","Iteration:  600, Loss: 509015.062500\n","Iteration:  610, Loss: 509015.031250\n","Iteration:  620, Loss: 509015.062500\n","Iteration:  630, Loss: 509015.062500\n","Iteration:  640, Loss: 509015.062500\n","Iteration:  650, Loss: 509015.062500\n","Iteration:  660, Loss: 509015.062500\n","Iteration:  670, Loss: 509015.093750\n","Iteration:  680, Loss: 509015.093750\n","Iteration:  690, Loss: 509015.000000\n","Iteration:  700, Loss: 509015.031250\n","Iteration:  710, Loss: 509015.062500\n","Iteration:  720, Loss: 509015.062500\n","Iteration:  730, Loss: 509015.000000\n","Iteration:  740, Loss: 509015.062500\n","Iteration:  750, Loss: 509015.062500\n","Iteration:  760, Loss: 509015.093750\n","Iteration:  770, Loss: 509015.031250\n","Iteration:  780, Loss: 509015.031250\n","Iteration:  790, Loss: 509015.062500\n","Iteration:  800, Loss: 509015.062500\n","Iteration:  810, Loss: 509015.031250\n","Iteration:  820, Loss: 509015.031250\n","Iteration:  830, Loss: 509015.093750\n","Iteration:  840, Loss: 509015.062500\n","Iteration:  850, Loss: 509015.093750\n","Iteration:  860, Loss: 509015.125000\n","Iteration:  870, Loss: 509015.031250\n","Iteration:  880, Loss: 509015.062500\n","Iteration:  890, Loss: 509015.062500\n","Iteration:  900, Loss: 509015.093750\n","Iteration:  910, Loss: 509015.062500\n","Iteration:  920, Loss: 509015.031250\n","Iteration:  930, Loss: 509015.093750\n","Iteration:  940, Loss: 509015.093750\n","Iteration:  950, Loss: 509015.093750\n","Iteration:  960, Loss: 509015.031250\n","Iteration:  970, Loss: 509015.062500\n","Iteration:  980, Loss: 509015.125000\n","Iteration:  990, Loss: 509015.062500\n","Iteration: 1000, Loss: 509015.062500\n","Elapsed time: 0:03:49.830940\n","X is normalized\n","PaCMAP(n_neighbors=26, n_MN=13, n_FP=52, distance=euclidean, lr=1.0, n_iters=1000, apply_pca=True, opt_method='adam', verbose=True, intermediate=False, seed=None)\n","Finding pairs\n","Found nearest neighbor\n","Calculated sigma\n","Found scaled dist\n","Pairs sampled successfully.\n","((2959892, 2), (1479946, 2), (5919784, 2))\n","Initial Loss: 3658542.0\n","Iteration:   10, Loss: 2403555.000000\n","Iteration:   20, Loss: 2148927.000000\n","Iteration:   30, Loss: 2015183.125000\n","Iteration:   40, Loss: 1910960.750000\n","Iteration:   50, Loss: 1811714.500000\n","Iteration:   60, Loss: 1706441.000000\n","Iteration:   70, Loss: 1588231.500000\n","Iteration:   80, Loss: 1448071.500000\n","Iteration:   90, Loss: 1271628.750000\n","Iteration:  100, Loss: 1005731.625000\n","Iteration:  110, Loss: 1315835.875000\n","Iteration:  120, Loss: 1293551.750000\n","Iteration:  130, Loss: 1284167.250000\n","Iteration:  140, Loss: 1281011.500000\n","Iteration:  150, Loss: 1280414.000000\n","Iteration:  160, Loss: 1280567.000000\n","Iteration:  170, Loss: 1280623.125000\n","Iteration:  180, Loss: 1281360.000000\n","Iteration:  190, Loss: 1282050.250000\n","Iteration:  200, Loss: 1282360.625000\n","Iteration:  210, Loss: 540398.375000\n","Iteration:  220, Loss: 535311.125000\n","Iteration:  230, Loss: 530722.812500\n","Iteration:  240, Loss: 528320.312500\n","Iteration:  250, Loss: 526363.875000\n","Iteration:  260, Loss: 524661.750000\n","Iteration:  270, Loss: 523308.000000\n","Iteration:  280, Loss: 522325.656250\n","Iteration:  290, Loss: 521399.406250\n","Iteration:  300, Loss: 520436.875000\n","Iteration:  310, Loss: 519462.781250\n","Iteration:  320, Loss: 518478.406250\n","Iteration:  330, Loss: 517595.312500\n","Iteration:  340, Loss: 516837.250000\n","Iteration:  350, Loss: 516243.281250\n","Iteration:  360, Loss: 515809.468750\n","Iteration:  370, Loss: 515484.031250\n","Iteration:  380, Loss: 515255.125000\n","Iteration:  390, Loss: 515096.750000\n","Iteration:  400, Loss: 514953.250000\n","Iteration:  410, Loss: 514805.218750\n","Iteration:  420, Loss: 514635.468750\n","Iteration:  430, Loss: 514434.062500\n","Iteration:  440, Loss: 514212.875000\n","Iteration:  450, Loss: 513991.187500\n","Iteration:  460, Loss: 513773.437500\n","Iteration:  470, Loss: 513574.812500\n","Iteration:  480, Loss: 513398.093750\n","Iteration:  490, Loss: 513234.218750\n","Iteration:  500, Loss: 513073.937500\n","Iteration:  510, Loss: 512905.500000\n","Iteration:  520, Loss: 512739.906250\n","Iteration:  530, Loss: 512582.812500\n","Iteration:  540, Loss: 512440.750000\n","Iteration:  550, Loss: 512306.875000\n","Iteration:  560, Loss: 512181.312500\n","Iteration:  570, Loss: 512072.093750\n","Iteration:  580, Loss: 511980.687500\n","Iteration:  590, Loss: 511903.031250\n","Iteration:  600, Loss: 511836.750000\n","Iteration:  610, Loss: 511784.906250\n","Iteration:  620, Loss: 511743.937500\n","Iteration:  630, Loss: 511715.906250\n","Iteration:  640, Loss: 511700.218750\n","Iteration:  650, Loss: 511689.218750\n","Iteration:  660, Loss: 511682.312500\n","Iteration:  670, Loss: 511675.875000\n","Iteration:  680, Loss: 511672.625000\n","Iteration:  690, Loss: 511676.000000\n","Iteration:  700, Loss: 511682.593750\n","Iteration:  710, Loss: 511687.718750\n","Iteration:  720, Loss: 511696.500000\n","Iteration:  730, Loss: 511710.812500\n","Iteration:  740, Loss: 511724.406250\n","Iteration:  750, Loss: 511740.406250\n","Iteration:  760, Loss: 511752.562500\n","Iteration:  770, Loss: 511759.000000\n","Iteration:  780, Loss: 511759.781250\n","Iteration:  790, Loss: 511753.843750\n","Iteration:  800, Loss: 511748.187500\n","Iteration:  810, Loss: 511741.812500\n","Iteration:  820, Loss: 511736.718750\n","Iteration:  830, Loss: 511734.312500\n","Iteration:  840, Loss: 511731.750000\n","Iteration:  850, Loss: 511728.843750\n","Iteration:  860, Loss: 511722.687500\n","Iteration:  870, Loss: 511714.218750\n","Iteration:  880, Loss: 511703.625000\n","Iteration:  890, Loss: 511691.218750\n","Iteration:  900, Loss: 511676.250000\n","Iteration:  910, Loss: 511658.156250\n","Iteration:  920, Loss: 511637.125000\n","Iteration:  930, Loss: 511612.906250\n","Iteration:  940, Loss: 511585.750000\n","Iteration:  950, Loss: 511557.812500\n","Iteration:  960, Loss: 511527.812500\n","Iteration:  970, Loss: 511496.812500\n","Iteration:  980, Loss: 511464.687500\n","Iteration:  990, Loss: 511432.250000\n","Iteration: 1000, Loss: 511401.750000\n","Elapsed time: 1104.22s\n","X is normalized.\n","X is normalized.\n","Found nearest neighbor\n","Found scaled dist\n","(740012, 2)\n","Initial Loss: 1395940.75\n","Iteration:   10, Loss: 704577.625000\n","Iteration:   20, Loss: 356225.156250\n","Iteration:   30, Loss: 257621.171875\n","Iteration:   40, Loss: 221760.687500\n","Iteration:   50, Loss: 209394.328125\n","Iteration:   60, Loss: 206005.703125\n","Iteration:   70, Loss: 204485.406250\n","Iteration:   80, Loss: 204060.890625\n","Iteration:   90, Loss: 203827.359375\n","Iteration:  100, Loss: 203788.656250\n","Iteration:  110, Loss: 306681.187500\n","Iteration:  120, Loss: 306132.000000\n","Iteration:  130, Loss: 305988.000000\n","Iteration:  140, Loss: 305945.968750\n","Iteration:  150, Loss: 305917.875000\n","Iteration:  160, Loss: 305913.093750\n","Iteration:  170, Loss: 305901.781250\n","Iteration:  180, Loss: 305922.218750\n","Iteration:  190, Loss: 305924.375000\n","Iteration:  200, Loss: 305943.437500\n","Iteration:  210, Loss: 101954.507812\n","Iteration:  220, Loss: 101893.398438\n","Iteration:  230, Loss: 101880.367188\n","Iteration:  240, Loss: 101875.500000\n","Iteration:  250, Loss: 101873.351562\n","Iteration:  260, Loss: 101872.968750\n","Iteration:  270, Loss: 101872.601562\n","Iteration:  280, Loss: 101872.828125\n","Iteration:  290, Loss: 101872.625000\n","Iteration:  300, Loss: 101873.031250\n","Iteration:  310, Loss: 101872.875000\n","Iteration:  320, Loss: 101872.789062\n","Iteration:  330, Loss: 101872.906250\n","Iteration:  340, Loss: 101872.882812\n","Iteration:  350, Loss: 101872.882812\n","Iteration:  360, Loss: 101872.929688\n","Iteration:  370, Loss: 101872.937500\n","Iteration:  380, Loss: 101872.890625\n","Iteration:  390, Loss: 101872.921875\n","Iteration:  400, Loss: 101872.906250\n","Iteration:  410, Loss: 101872.921875\n","Iteration:  420, Loss: 101872.921875\n","Iteration:  430, Loss: 101872.921875\n","Iteration:  440, Loss: 101872.921875\n","Iteration:  450, Loss: 101872.906250\n","Iteration:  460, Loss: 101872.921875\n","Iteration:  470, Loss: 101872.914062\n","Iteration:  480, Loss: 101872.921875\n","Iteration:  490, Loss: 101872.914062\n","Iteration:  500, Loss: 101872.929688\n","Iteration:  510, Loss: 101872.914062\n","Iteration:  520, Loss: 101872.921875\n","Iteration:  530, Loss: 101872.914062\n","Iteration:  540, Loss: 101872.906250\n","Iteration:  550, Loss: 101872.921875\n","Iteration:  560, Loss: 101872.914062\n","Iteration:  570, Loss: 101872.914062\n","Iteration:  580, Loss: 101872.921875\n","Iteration:  590, Loss: 101872.914062\n","Iteration:  600, Loss: 101872.914062\n","Iteration:  610, Loss: 101872.914062\n","Iteration:  620, Loss: 101872.914062\n","Iteration:  630, Loss: 101872.906250\n","Iteration:  640, Loss: 101872.914062\n","Iteration:  650, Loss: 101872.914062\n","Iteration:  660, Loss: 101872.914062\n","Iteration:  670, Loss: 101872.921875\n","Iteration:  680, Loss: 101872.914062\n","Iteration:  690, Loss: 101872.921875\n","Iteration:  700, Loss: 101872.921875\n","Iteration:  710, Loss: 101872.914062\n","Iteration:  720, Loss: 101872.921875\n","Iteration:  730, Loss: 101872.921875\n","Iteration:  740, Loss: 101872.914062\n","Iteration:  750, Loss: 101872.914062\n","Iteration:  760, Loss: 101872.921875\n","Iteration:  770, Loss: 101872.921875\n","Iteration:  780, Loss: 101872.914062\n","Iteration:  790, Loss: 101872.906250\n","Iteration:  800, Loss: 101872.921875\n","Iteration:  810, Loss: 101872.906250\n","Iteration:  820, Loss: 101872.921875\n","Iteration:  830, Loss: 101872.914062\n","Iteration:  840, Loss: 101872.914062\n","Iteration:  850, Loss: 101872.921875\n","Iteration:  860, Loss: 101872.906250\n","Iteration:  870, Loss: 101872.914062\n","Iteration:  880, Loss: 101872.914062\n","Iteration:  890, Loss: 101872.921875\n","Iteration:  900, Loss: 101872.921875\n","Iteration:  910, Loss: 101872.929688\n","Iteration:  920, Loss: 101872.914062\n","Iteration:  930, Loss: 101872.921875\n","Iteration:  940, Loss: 101872.921875\n","Iteration:  950, Loss: 101872.914062\n","Iteration:  960, Loss: 101872.921875\n","Iteration:  970, Loss: 101872.921875\n","Iteration:  980, Loss: 101872.914062\n","Iteration:  990, Loss: 101872.906250\n","Iteration: 1000, Loss: 101872.914062\n","Elapsed time: 0:01:00.668893\n","X is normalized.\n","X is normalized.\n","Found nearest neighbor\n","Found scaled dist\n","(3705078, 2)\n","Initial Loss: 7102855.0\n","Iteration:   10, Loss: 3495665.500000\n","Iteration:   20, Loss: 1786453.625000\n","Iteration:   30, Loss: 1296821.250000\n","Iteration:   40, Loss: 1106884.125000\n","Iteration:   50, Loss: 1041603.687500\n","Iteration:   60, Loss: 1027069.562500\n","Iteration:   70, Loss: 1021354.187500\n","Iteration:   80, Loss: 1019839.937500\n","Iteration:   90, Loss: 1019081.437500\n","Iteration:  100, Loss: 1018945.437500\n","Iteration:  110, Loss: 1518311.375000\n","Iteration:  120, Loss: 1514657.500000\n","Iteration:  130, Loss: 1513735.375000\n","Iteration:  140, Loss: 1513635.750000\n","Iteration:  150, Loss: 1513474.750000\n","Iteration:  160, Loss: 1513443.125000\n","Iteration:  170, Loss: 1513427.000000\n","Iteration:  180, Loss: 1513591.875000\n","Iteration:  190, Loss: 1513606.125000\n","Iteration:  200, Loss: 1513694.375000\n","Iteration:  210, Loss: 509697.281250\n","Iteration:  220, Loss: 509476.968750\n","Iteration:  230, Loss: 509427.687500\n","Iteration:  240, Loss: 509410.843750\n","Iteration:  250, Loss: 509405.218750\n","Iteration:  260, Loss: 509401.468750\n","Iteration:  270, Loss: 509401.812500\n","Iteration:  280, Loss: 509400.968750\n","Iteration:  290, Loss: 509400.625000\n","Iteration:  300, Loss: 509401.656250\n","Iteration:  310, Loss: 509401.281250\n","Iteration:  320, Loss: 509401.218750\n","Iteration:  330, Loss: 509400.625000\n","Iteration:  340, Loss: 509400.875000\n","Iteration:  350, Loss: 509400.812500\n","Iteration:  360, Loss: 509400.812500\n","Iteration:  370, Loss: 509400.875000\n","Iteration:  380, Loss: 509400.812500\n","Iteration:  390, Loss: 509400.843750\n","Iteration:  400, Loss: 509400.875000\n","Iteration:  410, Loss: 509400.875000\n","Iteration:  420, Loss: 509400.812500\n","Iteration:  430, Loss: 509400.843750\n","Iteration:  440, Loss: 509400.843750\n","Iteration:  450, Loss: 509400.843750\n","Iteration:  460, Loss: 509400.875000\n","Iteration:  470, Loss: 509400.875000\n","Iteration:  480, Loss: 509400.906250\n","Iteration:  490, Loss: 509400.843750\n","Iteration:  500, Loss: 509400.843750\n","Iteration:  510, Loss: 509400.843750\n","Iteration:  520, Loss: 509400.843750\n","Iteration:  530, Loss: 509400.875000\n","Iteration:  540, Loss: 509400.906250\n","Iteration:  550, Loss: 509400.875000\n","Iteration:  560, Loss: 509400.875000\n","Iteration:  570, Loss: 509400.843750\n","Iteration:  580, Loss: 509400.875000\n","Iteration:  590, Loss: 509400.843750\n","Iteration:  600, Loss: 509400.875000\n","Iteration:  610, Loss: 509400.875000\n","Iteration:  620, Loss: 509400.843750\n","Iteration:  630, Loss: 509400.843750\n","Iteration:  640, Loss: 509400.843750\n","Iteration:  650, Loss: 509400.875000\n","Iteration:  660, Loss: 509400.875000\n","Iteration:  670, Loss: 509400.875000\n","Iteration:  680, Loss: 509400.875000\n","Iteration:  690, Loss: 509400.875000\n","Iteration:  700, Loss: 509400.875000\n","Iteration:  710, Loss: 509400.843750\n","Iteration:  720, Loss: 509400.875000\n","Iteration:  730, Loss: 509400.875000\n","Iteration:  740, Loss: 509400.875000\n","Iteration:  750, Loss: 509400.875000\n","Iteration:  760, Loss: 509400.875000\n","Iteration:  770, Loss: 509400.875000\n","Iteration:  780, Loss: 509400.875000\n","Iteration:  790, Loss: 509400.843750\n","Iteration:  800, Loss: 509400.906250\n","Iteration:  810, Loss: 509400.875000\n","Iteration:  820, Loss: 509400.843750\n","Iteration:  830, Loss: 509400.843750\n","Iteration:  840, Loss: 509400.875000\n","Iteration:  850, Loss: 509400.875000\n","Iteration:  860, Loss: 509400.875000\n","Iteration:  870, Loss: 509400.843750\n","Iteration:  880, Loss: 509400.875000\n","Iteration:  890, Loss: 509400.843750\n","Iteration:  900, Loss: 509400.875000\n","Iteration:  910, Loss: 509400.812500\n","Iteration:  920, Loss: 509400.875000\n","Iteration:  930, Loss: 509400.906250\n","Iteration:  940, Loss: 509400.843750\n","Iteration:  950, Loss: 509400.875000\n","Iteration:  960, Loss: 509400.843750\n","Iteration:  970, Loss: 509400.812500\n","Iteration:  980, Loss: 509400.875000\n","Iteration:  990, Loss: 509400.843750\n","Iteration: 1000, Loss: 509400.843750\n","Elapsed time: 0:03:56.361722\n"]}]},{"cell_type":"code","source":["## pacmap과 isolation forest 1차 이용(2)\n","val_score_1 = f1_score(ori_val_df['Class'], np.round(val_pred_set_1), average='macro')\n","\n","print(f'Validation F1 Score : [{val_score_1}]')\n","print(classification_report(ori_val_df['Class'], np.round(val_pred_set_1)))\n","print(confusion_matrix(ori_val_df['Class'], np.round(val_pred_set_1)))"],"metadata":{"id":"F4P1p5YVoc-s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660110723315,"user_tz":-540,"elapsed":320,"user":{"displayName":"­신기수","userId":"10948147504762254674"}},"outputId":"8d8fb95f-60f7-4df5-f25a-d83b784047e4"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation F1 Score : [0.9209734995691702]\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     28432\n","           1       0.89      0.80      0.84        30\n","\n","    accuracy                           1.00     28462\n","   macro avg       0.94      0.90      0.92     28462\n","weighted avg       1.00      1.00      1.00     28462\n","\n","[[28429     3]\n"," [    6    24]]\n"]}]},{"cell_type":"code","source":["## pacmap과 isolation forest 1차 이용(3)\n","# 1차 저장\n","chujung_train_1 = pd.DataFrame({'Class':np.round(train_pred_set_1)})\n","chujung_val_1 = pd.DataFrame({'Class':np.round(val_pred_set_1)})\n","chujung_test_1 = pd.DataFrame({'Class':np.round(test_pred_set_1)})\n","\n","result_train_1 = pd.concat([train_df,chujung_train_1], axis=1)\n","result_val_1 = pd.concat([val_df,chujung_val_1], axis=1)\n","result_test_1 = pd.concat([test_df,chujung_test_1], axis=1)\n","\n","result_train_1.to_csv('result_train_1.csv', index=False)\n","result_val_1.to_csv('result_val_1.csv', index=False)\n","result_test_1.to_csv('result_test_1.csv', index=False)"],"metadata":{"id":"3rvSp1cCDcAR","executionInfo":{"status":"ok","timestamp":1660110753266,"user_tz":-540,"elapsed":11806,"user":{"displayName":"­신기수","userId":"10948147504762254674"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["## pacmap과 isolation forest 1차 이용(4)\n","# 1차 불러오기\n","train_pred_set_1 = np.array(pd.read_csv('result_train_1.csv')['Class'])\n","val_pred_set_1 = np.array(pd.read_csv('result_val_1.csv')['Class'])\n","test_pred_set_1 = np.array(pd.read_csv('result_test_1.csv')['Class'])"],"metadata":{"id":"nSNfIUcHOLnA","executionInfo":{"status":"ok","timestamp":1660126058622,"user_tz":-540,"elapsed":6894,"user":{"displayName":"­신기수","userId":"10948147504762254674"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["저는 이에 더해 변수를 좀 더 추가하면서 val score를 높일 수 있을까 생각하였습니다. 그래서 저는 설령 outlier들이 inlier안에 숨어있을지라도 inlier들이 모여있으면 isolation forest가 판단하는데 있어서 도움이 된다고 들었습니다. 이에 따라 저는 outlier들이 inlier안에 들어있지만 최대한 outlier들의 분산이 적으면서 최대한 inlier들의 분산을 넓은 변수를 찾고자 하였습니다. (i.e. (inlier들의 분산/outlier들의 분산)이 높은 것들). 역시 validation set의 통계정보를 이용하였습니다. 그 다음, outlier의 중앙값을 기준으로 inlier들을 나누어서, 각 inlier들의 중앙값이 outlier의 중앙값과 차이가 큰지(wilcoxon rank sum test), 각 inlier들의 분산이 작은지(해당 분산이 작으면 inlier와 구별할 수 있다고 생각하였습니다.)를 보았습니다. 변수를 최대한 적게 선택하고자 이번엔 해당 방법들에서 나온 변수들의 교집합을 선택하였습니다."],"metadata":{"id":"mhAM8EbO5an4"}},{"cell_type":"code","source":["## pacmap과 isolation forest 2차 이용\n","inside_in_inlier= []\n","\n","for what_val in range(30):\n","    if ori_val_df.iloc[np.where(ori_val_df['Class'] == 1)].max()[what_val] <  ori_val_df.iloc[np.where(ori_val_df['Class'] == 0)].max()[what_val]:\n","        if ori_val_df.iloc[np.where(ori_val_df['Class'] == 1)].min()[what_val] >  ori_val_df.iloc[np.where(ori_val_df['Class'] == 0)].min()[what_val]:\n","            inside_in_inlier.append(what_val)\n","\n","print(inside_in_inlier)\n","\n","var_chai = []\n","for what_val in inside_in_inlier:\n","    print(what_val , ori_val_df.iloc[np.where(ori_val_df['Class'] == 0)[0],what_val].var()/ori_val_df.iloc[np.where(ori_val_df['Class'] == 1)[0],what_val].var())\n","    var_chai.append(ori_val_df.iloc[np.where(ori_val_df['Class'] == 0)[0],what_val].var()/ori_val_df.iloc[np.where(ori_val_df['Class'] == 1)[0],what_val].var())\n","\n","old_born_idx = np.argsort((-1)*np.array(var_chai))[:5]\n","old_born_idx = np.array(inside_in_inlier)[list(old_born_idx)]\n","old_born_idx = list(old_born_idx)\n","print(old_born_idx)\n","\n","left_side_l = []\n","right_side_l = []\n","for jjkk in inside_in_inlier:\n","    the_one_the_one = ori_val_df.iloc[np.where(ori_val_df['Class'] == 1)[0],jjkk]\n","    median_the_one = the_one_the_one.median()\n","    the_zero_the_zero = ori_val_df.iloc[np.where(ori_val_df['Class'] == 0)[0],jjkk]\n","    left_zero_val_df = the_zero_the_zero.iloc[np.where(the_zero_the_zero < median_the_one)[0]]\n","    right_zero_val_df = the_zero_the_zero.iloc[np.where(the_zero_the_zero > median_the_one)[0]]\n","    left_ppp = (ranksums(left_zero_val_df, the_one_the_one).pvalue)\n","    right_ppp = (ranksums(right_zero_val_df, the_one_the_one).pvalue)\n","    \n","    left_side_l.append((left_ppp*1000)*((left_zero_val_df.var())))\n","    right_side_l.append((right_ppp*1000)*((right_zero_val_df.var())))\n","\n","left_born_idx = np.argsort(np.array(left_side_l))[:5]\n","left_born_idx = np.array(inside_in_inlier)[list(left_born_idx)]\n","left_born_idx = list(left_born_idx)\n","\n","right_born_idx = np.argsort(np.array(right_side_l))[:5]\n","right_born_idx = np.array(inside_in_inlier)[list(right_born_idx)]\n","right_born_idx = list(right_born_idx)\n","\n","print(left_born_idx)\n","print(right_born_idx)\n","\n","dhk_add_list = list(set(old_born_idx) & set(left_born_idx) & set(right_born_idx))\n","print(dhk_add_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rRX6ILqXa0c-","executionInfo":{"status":"ok","timestamp":1660126063385,"user_tz":-540,"elapsed":3211,"user":{"displayName":"­신기수","userId":"10948147504762254674"}},"outputId":"10181864-8fc5-4121-d61d-1ad205223484"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 4, 5, 12, 14, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29]\n","0 0.05661145916743179\n","4 0.04223885928210881\n","5 0.7704461524323427\n","12 0.7649192537628348\n","14 0.592450840972954\n","18 0.20398714518238045\n","19 0.3715199816175286\n","20 0.0668143847139044\n","21 0.47076015282831585\n","22 0.516526376951607\n","23 1.3711184741040796\n","24 0.4679685966190287\n","25 0.7132699704559123\n","26 0.037185842989192905\n","27 0.2518467335441023\n","29 0.782343614593578\n","[23, 29, 5, 12, 25]\n","[25, 21, 23, 24, 20]\n","[5, 0, 24, 23, 29]\n","[23]\n"]}]},{"cell_type":"markdown","source":["지금까지 선택한 변수들을 바탕으로 pacmap과 isolation forest를 돌려 예측합니다."],"metadata":{"id":"nw7wdNha69Gv"}},{"cell_type":"code","source":["## pacmap과 isolation forest 3차 이용(1)\n","# pacmap과 isolation forest를 이용한 2차 예측\n","hhmm = 7\n","vlskffo_var = superior_var2 + dhk_add_list\n","what_val = vlskffo_var\n","\n","for num in range(hhmm):\n","    embedding_3 = pacmap.PaCMAP(n_components=len(what_val), n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, num_iters = 600, verbose = True)\n","    pacmac_train_3 = embedding_3.fit_transform(np.array(train_df.iloc[:,what_val]), init=\"pca\")\n","    pacmac_val_3 = embedding_3.transform(np.array(val_df.iloc[:,what_val]), basis=np.array(train_df.iloc[:,what_val]))\n","    pacmac_test_3 = embedding_3.transform(np.array(test_df.iloc[:,what_val]), basis=np.array(train_df.iloc[:,what_val]))\n","\n","    pac_model_3 = IsolationForest(n_estimators=1000, contamination=0.00121, verbose=0)\n","    pac_model_3.fit(pacmac_train_3[:,[1,2,3]]) \n","\n","    if num == 0:\n","        train_pred_set_3 = pac_model_3.predict(pacmac_train_3[:,[1,2,3]]) # model prediction\n","        train_pred_set_3 = get_pred_label(train_pred_set_3)\n","\n","        val_pred_set_3 = pac_model_3.predict(pacmac_val_3[:,[1,2,3]]) # model prediction\n","        val_pred_set_3 = get_pred_label(val_pred_set_3)\n","\n","        test_pred_set_3 = pac_model_3.predict(pacmac_test_3[:,[1,2,3]]) # model prediction\n","        test_pred_set_3 = get_pred_label(test_pred_set_3)\n","    else:\n","        train_pred_3 = pac_model_3.predict(pacmac_train_3[:,[1,2,3]]) # model prediction\n","        train_pred_3 = get_pred_label(train_pred_3)\n","        train_pred_set_3 = train_pred_set_3 + train_pred_3\n","\n","        val_pred_3 = pac_model_3.predict(pacmac_val_3[:,[1,2,3]]) # model prediction\n","        val_pred_3 = get_pred_label(val_pred_3)\n","        val_pred_set_3 = val_pred_set_3 + val_pred_3\n","\n","        test_pred_3 = pac_model_3.predict(pacmac_test_3[:,[1,2,3]]) # model prediction\n","        test_pred_3 = get_pred_label(test_pred_3)\n","        test_pred_set_3 = test_pred_set_3 + test_pred_3\n","\n","train_pred_set_3 = train_pred_set_3/hhmm\n","val_pred_set_3 = val_pred_set_3/hhmm\n","test_pred_set_3 = test_pred_set_3/hhmm"],"metadata":{"id":"tDnZmwISEkS6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660136009507,"user_tz":-540,"elapsed":9942720,"user":{"displayName":"­신기수","userId":"10948147504762254674"}},"outputId":"85b1d969-c8af-4136-b179-1411cc3bbdb0"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["X is normalized\n","PaCMAP(n_neighbors=26, n_MN=13, n_FP=52, distance=euclidean, lr=1.0, n_iters=600, apply_pca=True, opt_method='adam', verbose=True, intermediate=False, seed=None)\n","Finding pairs\n","Found nearest neighbor\n","Calculated sigma\n","Found scaled dist\n","Pairs sampled successfully.\n","((2959892, 2), (1479946, 2), (5919784, 2))\n","Initial Loss: 3658542.0\n","Iteration:   10, Loss: 2461965.750000\n","Iteration:   20, Loss: 2168983.250000\n","Iteration:   30, Loss: 2027381.000000\n","Iteration:   40, Loss: 1920832.500000\n","Iteration:   50, Loss: 1820802.500000\n","Iteration:   60, Loss: 1715505.375000\n","Iteration:   70, Loss: 1597789.750000\n","Iteration:   80, Loss: 1458907.625000\n","Iteration:   90, Loss: 1283804.875000\n","Iteration:  100, Loss: 1020497.750000\n","Iteration:  110, Loss: 1339413.250000\n","Iteration:  120, Loss: 1314553.250000\n","Iteration:  130, Loss: 1303085.125000\n","Iteration:  140, Loss: 1299084.250000\n","Iteration:  150, Loss: 1298269.625000\n","Iteration:  160, Loss: 1298299.000000\n","Iteration:  170, Loss: 1298458.125000\n","Iteration:  180, Loss: 1299243.500000\n","Iteration:  190, Loss: 1300156.625000\n","Iteration:  200, Loss: 1300722.500000\n","Iteration:  210, Loss: 549986.125000\n","Iteration:  220, Loss: 543932.312500\n","Iteration:  230, Loss: 539219.125000\n","Iteration:  240, Loss: 536566.750000\n","Iteration:  250, Loss: 534438.437500\n","Iteration:  260, Loss: 532775.187500\n","Iteration:  270, Loss: 531467.437500\n","Iteration:  280, Loss: 530408.187500\n","Iteration:  290, Loss: 529521.625000\n","Iteration:  300, Loss: 528611.187500\n","Iteration:  310, Loss: 527659.500000\n","Iteration:  320, Loss: 526627.875000\n","Iteration:  330, Loss: 525696.250000\n","Iteration:  340, Loss: 524872.812500\n","Iteration:  350, Loss: 524180.093750\n","Iteration:  360, Loss: 523657.781250\n","Iteration:  370, Loss: 523254.500000\n","Iteration:  380, Loss: 522979.625000\n","Iteration:  390, Loss: 522765.906250\n","Iteration:  400, Loss: 522551.218750\n","Iteration:  410, Loss: 522303.062500\n","Iteration:  420, Loss: 522042.562500\n","Iteration:  430, Loss: 521776.125000\n","Iteration:  440, Loss: 521522.343750\n","Iteration:  450, Loss: 521289.531250\n","Iteration:  460, Loss: 521066.625000\n","Iteration:  470, Loss: 520869.031250\n","Iteration:  480, Loss: 520682.375000\n","Iteration:  490, Loss: 520499.000000\n","Iteration:  500, Loss: 520321.343750\n","Iteration:  510, Loss: 520133.312500\n","Iteration:  520, Loss: 519958.687500\n","Iteration:  530, Loss: 519784.281250\n","Iteration:  540, Loss: 519610.375000\n","Iteration:  550, Loss: 519443.437500\n","Iteration:  560, Loss: 519288.281250\n","Iteration:  570, Loss: 519149.156250\n","Iteration:  580, Loss: 519032.281250\n","Iteration:  590, Loss: 518931.875000\n","Iteration:  600, Loss: 518849.093750\n","Elapsed time: 911.68s\n","X is normalized.\n","X is normalized.\n","Found nearest neighbor\n","Found scaled dist\n","(740012, 2)\n","Initial Loss: 1365338.0\n","Iteration:   10, Loss: 675590.062500\n","Iteration:   20, Loss: 323601.031250\n","Iteration:   30, Loss: 238764.250000\n","Iteration:   40, Loss: 217506.625000\n","Iteration:   50, Loss: 210736.765625\n","Iteration:   60, Loss: 207014.000000\n","Iteration:   70, Loss: 206251.609375\n","Iteration:   80, Loss: 205791.609375\n","Iteration:   90, Loss: 205692.328125\n","Iteration:  100, Loss: 205629.453125\n","Iteration:  110, Loss: 310007.531250\n","Iteration:  120, Loss: 309143.093750\n","Iteration:  130, Loss: 308889.531250\n","Iteration:  140, Loss: 308831.156250\n","Iteration:  150, Loss: 308780.593750\n","Iteration:  160, Loss: 308746.968750\n","Iteration:  170, Loss: 308741.250000\n","Iteration:  180, Loss: 308766.843750\n","Iteration:  190, Loss: 308782.687500\n","Iteration:  200, Loss: 308828.093750\n","Iteration:  210, Loss: 102906.281250\n","Iteration:  220, Loss: 102827.101562\n","Iteration:  230, Loss: 102811.382812\n","Iteration:  240, Loss: 102805.593750\n","Iteration:  250, Loss: 102803.367188\n","Iteration:  260, Loss: 102802.960938\n","Iteration:  270, Loss: 102802.273438\n","Iteration:  280, Loss: 102802.125000\n","Iteration:  290, Loss: 102802.046875\n","Iteration:  300, Loss: 102801.929688\n","Iteration:  310, Loss: 102801.929688\n","Iteration:  320, Loss: 102801.968750\n","Iteration:  330, Loss: 102801.976562\n","Iteration:  340, Loss: 102801.984375\n","Iteration:  350, Loss: 102801.937500\n","Iteration:  360, Loss: 102801.929688\n","Iteration:  370, Loss: 102801.945312\n","Iteration:  380, Loss: 102801.945312\n","Iteration:  390, Loss: 102801.945312\n","Iteration:  400, Loss: 102801.945312\n","Iteration:  410, Loss: 102801.945312\n","Iteration:  420, Loss: 102801.937500\n","Iteration:  430, Loss: 102801.937500\n","Iteration:  440, Loss: 102801.945312\n","Iteration:  450, Loss: 102801.945312\n","Iteration:  460, Loss: 102801.945312\n","Iteration:  470, Loss: 102801.937500\n","Iteration:  480, Loss: 102801.945312\n","Iteration:  490, Loss: 102801.937500\n","Iteration:  500, Loss: 102801.937500\n","Iteration:  510, Loss: 102801.937500\n","Iteration:  520, Loss: 102801.937500\n","Iteration:  530, Loss: 102801.937500\n","Iteration:  540, Loss: 102801.945312\n","Iteration:  550, Loss: 102801.945312\n","Iteration:  560, Loss: 102801.929688\n","Iteration:  570, Loss: 102801.945312\n","Iteration:  580, Loss: 102801.945312\n","Iteration:  590, Loss: 102801.945312\n","Iteration:  600, Loss: 102801.937500\n","Elapsed time: 0:00:40.431669\n","X is normalized.\n","X is normalized.\n","Found nearest neighbor\n","Found scaled dist\n","(3705078, 2)\n","Initial Loss: 6933824.0\n","Iteration:   10, Loss: 3384260.500000\n","Iteration:   20, Loss: 1630240.625000\n","Iteration:   30, Loss: 1201341.125000\n","Iteration:   40, Loss: 1085651.625000\n","Iteration:   50, Loss: 1047959.750000\n","Iteration:   60, Loss: 1033183.187500\n","Iteration:   70, Loss: 1030661.187500\n","Iteration:   80, Loss: 1029078.250000\n","Iteration:   90, Loss: 1028720.687500\n","Iteration:  100, Loss: 1028520.562500\n","Iteration:  110, Loss: 1533866.625000\n","Iteration:  120, Loss: 1527293.250000\n","Iteration:  130, Loss: 1525564.250000\n","Iteration:  140, Loss: 1525274.500000\n","Iteration:  150, Loss: 1525002.125000\n","Iteration:  160, Loss: 1524985.375000\n","Iteration:  170, Loss: 1525017.875000\n","Iteration:  180, Loss: 1525114.625000\n","Iteration:  190, Loss: 1525238.500000\n","Iteration:  200, Loss: 1525450.875000\n","Iteration:  210, Loss: 514552.218750\n","Iteration:  220, Loss: 514312.062500\n","Iteration:  230, Loss: 514259.281250\n","Iteration:  240, Loss: 514238.406250\n","Iteration:  250, Loss: 514233.406250\n","Iteration:  260, Loss: 514231.062500\n","Iteration:  270, Loss: 514230.500000\n","Iteration:  280, Loss: 514230.031250\n","Iteration:  290, Loss: 514230.500000\n","Iteration:  300, Loss: 514230.218750\n","Iteration:  310, Loss: 514229.500000\n","Iteration:  320, Loss: 514230.062500\n","Iteration:  330, Loss: 514230.531250\n","Iteration:  340, Loss: 514230.437500\n","Iteration:  350, Loss: 514230.500000\n","Iteration:  360, Loss: 514230.562500\n","Iteration:  370, Loss: 514230.687500\n","Iteration:  380, Loss: 514230.593750\n","Iteration:  390, Loss: 514230.625000\n","Iteration:  400, Loss: 514230.562500\n","Iteration:  410, Loss: 514230.625000\n","Iteration:  420, Loss: 514230.656250\n","Iteration:  430, Loss: 514230.656250\n","Iteration:  440, Loss: 514230.625000\n","Iteration:  450, Loss: 514230.593750\n","Iteration:  460, Loss: 514230.625000\n","Iteration:  470, Loss: 514230.625000\n","Iteration:  480, Loss: 514230.593750\n","Iteration:  490, Loss: 514230.625000\n","Iteration:  500, Loss: 514230.656250\n","Iteration:  510, Loss: 514230.625000\n","Iteration:  520, Loss: 514230.687500\n","Iteration:  530, Loss: 514230.625000\n","Iteration:  540, Loss: 514230.656250\n","Iteration:  550, Loss: 514230.656250\n","Iteration:  560, Loss: 514230.625000\n","Iteration:  570, Loss: 514230.656250\n","Iteration:  580, Loss: 514230.593750\n","Iteration:  590, Loss: 514230.625000\n","Iteration:  600, Loss: 514230.687500\n","Elapsed time: 0:02:44.499218\n","X is normalized\n","PaCMAP(n_neighbors=26, n_MN=13, n_FP=52, distance=euclidean, lr=1.0, n_iters=600, apply_pca=True, opt_method='adam', verbose=True, intermediate=False, seed=None)\n","Finding pairs\n","Found nearest neighbor\n","Calculated sigma\n","Found scaled dist\n","Pairs sampled successfully.\n","((2959892, 2), (1479946, 2), (5919784, 2))\n","Initial Loss: 3658542.0\n","Iteration:   10, Loss: 2462045.500000\n","Iteration:   20, Loss: 2168747.750000\n","Iteration:   30, Loss: 2027344.750000\n","Iteration:   40, Loss: 1920677.000000\n","Iteration:   50, Loss: 1820607.500000\n","Iteration:   60, Loss: 1715271.000000\n","Iteration:   70, Loss: 1597649.875000\n","Iteration:   80, Loss: 1458785.625000\n","Iteration:   90, Loss: 1283516.500000\n","Iteration:  100, Loss: 1020034.125000\n","Iteration:  110, Loss: 1339046.625000\n","Iteration:  120, Loss: 1314335.625000\n","Iteration:  130, Loss: 1302929.500000\n","Iteration:  140, Loss: 1298835.000000\n","Iteration:  150, Loss: 1297918.625000\n","Iteration:  160, Loss: 1297858.375000\n","Iteration:  170, Loss: 1297850.125000\n","Iteration:  180, Loss: 1298589.500000\n","Iteration:  190, Loss: 1299528.750000\n","Iteration:  200, Loss: 1300070.750000\n","Iteration:  210, Loss: 549859.312500\n","Iteration:  220, Loss: 543786.500000\n","Iteration:  230, Loss: 539140.437500\n","Iteration:  240, Loss: 536492.062500\n","Iteration:  250, Loss: 534327.437500\n","Iteration:  260, Loss: 532601.625000\n","Iteration:  270, Loss: 531297.125000\n","Iteration:  280, Loss: 530297.375000\n","Iteration:  290, Loss: 529453.875000\n","Iteration:  300, Loss: 528561.062500\n","Iteration:  310, Loss: 527600.937500\n","Iteration:  320, Loss: 526576.812500\n","Iteration:  330, Loss: 525619.187500\n","Iteration:  340, Loss: 524785.125000\n","Iteration:  350, Loss: 524086.468750\n","Iteration:  360, Loss: 523577.625000\n","Iteration:  370, Loss: 523187.906250\n","Iteration:  380, Loss: 522932.875000\n","Iteration:  390, Loss: 522723.406250\n","Iteration:  400, Loss: 522492.718750\n","Iteration:  410, Loss: 522239.125000\n","Iteration:  420, Loss: 521957.312500\n","Iteration:  430, Loss: 521679.312500\n","Iteration:  440, Loss: 521420.593750\n","Iteration:  450, Loss: 521178.937500\n","Iteration:  460, Loss: 520957.656250\n","Iteration:  470, Loss: 520757.406250\n","Iteration:  480, Loss: 520579.656250\n","Iteration:  490, Loss: 520403.531250\n","Iteration:  500, Loss: 520227.937500\n","Iteration:  510, Loss: 520050.781250\n","Iteration:  520, Loss: 519870.062500\n","Iteration:  530, Loss: 519699.843750\n","Iteration:  540, Loss: 519518.656250\n","Iteration:  550, Loss: 519345.812500\n","Iteration:  560, Loss: 519186.187500\n","Iteration:  570, Loss: 519051.343750\n","Iteration:  580, Loss: 518938.218750\n","Iteration:  590, Loss: 518839.093750\n","Iteration:  600, Loss: 518761.687500\n","Elapsed time: 942.89s\n","X is normalized.\n","X is normalized.\n","Found nearest neighbor\n","Found scaled dist\n","(740012, 2)\n","Initial Loss: 1365048.625\n","Iteration:   10, Loss: 677445.000000\n","Iteration:   20, Loss: 324093.812500\n","Iteration:   30, Loss: 239163.187500\n","Iteration:   40, Loss: 217828.828125\n","Iteration:   50, Loss: 210819.750000\n","Iteration:   60, Loss: 207195.640625\n","Iteration:   70, Loss: 206415.875000\n","Iteration:   80, Loss: 205961.093750\n","Iteration:   90, Loss: 205856.765625\n","Iteration:  100, Loss: 205794.218750\n","Iteration:  110, Loss: 310332.531250\n","Iteration:  120, Loss: 309412.000000\n","Iteration:  130, Loss: 309139.000000\n","Iteration:  140, Loss: 309078.437500\n","Iteration:  150, Loss: 309030.156250\n","Iteration:  160, Loss: 309004.343750\n","Iteration:  170, Loss: 309008.343750\n","Iteration:  180, Loss: 309008.875000\n","Iteration:  190, Loss: 309022.437500\n","Iteration:  200, Loss: 309032.093750\n","Iteration:  210, Loss: 102981.906250\n","Iteration:  220, Loss: 102907.140625\n","Iteration:  230, Loss: 102892.164062\n","Iteration:  240, Loss: 102887.460938\n","Iteration:  250, Loss: 102885.882812\n","Iteration:  260, Loss: 102884.945312\n","Iteration:  270, Loss: 102884.664062\n","Iteration:  280, Loss: 102884.609375\n","Iteration:  290, Loss: 102884.687500\n","Iteration:  300, Loss: 102884.882812\n","Iteration:  310, Loss: 102884.773438\n","Iteration:  320, Loss: 102884.773438\n","Iteration:  330, Loss: 102884.781250\n","Iteration:  340, Loss: 102884.828125\n","Iteration:  350, Loss: 102884.773438\n","Iteration:  360, Loss: 102884.796875\n","Iteration:  370, Loss: 102884.812500\n","Iteration:  380, Loss: 102884.804688\n","Iteration:  390, Loss: 102884.812500\n","Iteration:  400, Loss: 102884.804688\n","Iteration:  410, Loss: 102884.796875\n","Iteration:  420, Loss: 102884.812500\n","Iteration:  430, Loss: 102884.804688\n","Iteration:  440, Loss: 102884.804688\n","Iteration:  450, Loss: 102884.789062\n","Iteration:  460, Loss: 102884.796875\n","Iteration:  470, Loss: 102884.804688\n","Iteration:  480, Loss: 102884.804688\n","Iteration:  490, Loss: 102884.804688\n","Iteration:  500, Loss: 102884.812500\n","Iteration:  510, Loss: 102884.796875\n","Iteration:  520, Loss: 102884.796875\n","Iteration:  530, Loss: 102884.796875\n","Iteration:  540, Loss: 102884.796875\n","Iteration:  550, Loss: 102884.804688\n","Iteration:  560, Loss: 102884.804688\n","Iteration:  570, Loss: 102884.796875\n","Iteration:  580, Loss: 102884.796875\n","Iteration:  590, Loss: 102884.804688\n","Iteration:  600, Loss: 102884.796875\n","Elapsed time: 0:00:40.084407\n","X is normalized.\n","X is normalized.\n","Found nearest neighbor\n","Found scaled dist\n","(3705078, 2)\n","Initial Loss: 6929266.5\n","Iteration:   10, Loss: 3395987.500000\n","Iteration:   20, Loss: 1632886.250000\n","Iteration:   30, Loss: 1203892.875000\n","Iteration:   40, Loss: 1085633.250000\n","Iteration:   50, Loss: 1047961.187500\n","Iteration:   60, Loss: 1033505.875000\n","Iteration:   70, Loss: 1030926.750000\n","Iteration:   80, Loss: 1029331.562500\n","Iteration:   90, Loss: 1028974.437500\n","Iteration:  100, Loss: 1028798.062500\n","Iteration:  110, Loss: 1534873.250000\n","Iteration:  120, Loss: 1527892.250000\n","Iteration:  130, Loss: 1526059.000000\n","Iteration:  140, Loss: 1525705.500000\n","Iteration:  150, Loss: 1525543.375000\n","Iteration:  160, Loss: 1525397.875000\n","Iteration:  170, Loss: 1525413.125000\n","Iteration:  180, Loss: 1525416.125000\n","Iteration:  190, Loss: 1525518.750000\n","Iteration:  200, Loss: 1525655.250000\n","Iteration:  210, Loss: 514664.187500\n","Iteration:  220, Loss: 514426.812500\n","Iteration:  230, Loss: 514374.656250\n","Iteration:  240, Loss: 514357.906250\n","Iteration:  250, Loss: 514352.187500\n","Iteration:  260, Loss: 514352.531250\n","Iteration:  270, Loss: 514350.656250\n","Iteration:  280, Loss: 514350.000000\n","Iteration:  290, Loss: 514350.468750\n","Iteration:  300, Loss: 514350.250000\n","Iteration:  310, Loss: 514350.687500\n","Iteration:  320, Loss: 514351.000000\n","Iteration:  330, Loss: 514351.250000\n","Iteration:  340, Loss: 514350.937500\n","Iteration:  350, Loss: 514350.843750\n","Iteration:  360, Loss: 514350.906250\n","Iteration:  370, Loss: 514351.000000\n","Iteration:  380, Loss: 514350.875000\n","Iteration:  390, Loss: 514350.906250\n","Iteration:  400, Loss: 514350.906250\n","Iteration:  410, Loss: 514350.875000\n","Iteration:  420, Loss: 514350.906250\n","Iteration:  430, Loss: 514350.875000\n","Iteration:  440, Loss: 514350.875000\n","Iteration:  450, Loss: 514350.875000\n","Iteration:  460, Loss: 514350.875000\n","Iteration:  470, Loss: 514350.906250\n","Iteration:  480, Loss: 514350.875000\n","Iteration:  490, Loss: 514350.875000\n","Iteration:  500, Loss: 514350.875000\n","Iteration:  510, Loss: 514350.906250\n","Iteration:  520, Loss: 514350.937500\n","Iteration:  530, Loss: 514350.906250\n","Iteration:  540, Loss: 514350.875000\n","Iteration:  550, Loss: 514350.875000\n","Iteration:  560, Loss: 514350.875000\n","Iteration:  570, Loss: 514350.875000\n","Iteration:  580, Loss: 514350.875000\n","Iteration:  590, Loss: 514350.875000\n","Iteration:  600, Loss: 514350.875000\n","Elapsed time: 0:02:47.043866\n","X is normalized\n","PaCMAP(n_neighbors=26, n_MN=13, n_FP=52, distance=euclidean, lr=1.0, n_iters=600, apply_pca=True, opt_method='adam', verbose=True, intermediate=False, seed=None)\n","Finding pairs\n","Found nearest neighbor\n","Calculated sigma\n","Found scaled dist\n","Pairs sampled successfully.\n","((2959892, 2), (1479946, 2), (5919784, 2))\n","Initial Loss: 3658542.0\n","Iteration:   10, Loss: 2463141.500000\n","Iteration:   20, Loss: 2168962.500000\n","Iteration:   30, Loss: 2027352.625000\n","Iteration:   40, Loss: 1920645.250000\n","Iteration:   50, Loss: 1820639.750000\n","Iteration:   60, Loss: 1715341.750000\n","Iteration:   70, Loss: 1597671.000000\n","Iteration:   80, Loss: 1458820.000000\n","Iteration:   90, Loss: 1283625.750000\n","Iteration:  100, Loss: 1020221.750000\n","Iteration:  110, Loss: 1339003.000000\n","Iteration:  120, Loss: 1314251.750000\n","Iteration:  130, Loss: 1302892.000000\n","Iteration:  140, Loss: 1298997.500000\n","Iteration:  150, Loss: 1298443.000000\n","Iteration:  160, Loss: 1298564.875000\n","Iteration:  170, Loss: 1298814.250000\n","Iteration:  180, Loss: 1299724.500000\n","Iteration:  190, Loss: 1300756.250000\n","Iteration:  200, Loss: 1301242.750000\n","Iteration:  210, Loss: 550095.062500\n","Iteration:  220, Loss: 544099.500000\n","Iteration:  230, Loss: 539303.875000\n","Iteration:  240, Loss: 536709.375000\n","Iteration:  250, Loss: 534746.812500\n","Iteration:  260, Loss: 533098.062500\n","Iteration:  270, Loss: 531798.125000\n","Iteration:  280, Loss: 530704.687500\n","Iteration:  290, Loss: 529764.687500\n","Iteration:  300, Loss: 528748.937500\n","Iteration:  310, Loss: 527745.500000\n","Iteration:  320, Loss: 526700.250000\n","Iteration:  330, Loss: 525737.562500\n","Iteration:  340, Loss: 524900.500000\n","Iteration:  350, Loss: 524184.031250\n","Iteration:  360, Loss: 523646.250000\n","Iteration:  370, Loss: 523243.812500\n","Iteration:  380, Loss: 522977.062500\n","Iteration:  390, Loss: 522769.750000\n","Iteration:  400, Loss: 522573.375000\n","Iteration:  410, Loss: 522348.437500\n","Iteration:  420, Loss: 522120.500000\n","Iteration:  430, Loss: 521880.093750\n","Iteration:  440, Loss: 521647.562500\n","Iteration:  450, Loss: 521413.312500\n","Iteration:  460, Loss: 521175.031250\n","Iteration:  470, Loss: 520940.468750\n","Iteration:  480, Loss: 520730.562500\n","Iteration:  490, Loss: 520525.093750\n","Iteration:  500, Loss: 520323.593750\n","Iteration:  510, Loss: 520126.593750\n","Iteration:  520, Loss: 519949.437500\n","Iteration:  530, Loss: 519767.937500\n","Iteration:  540, Loss: 519589.781250\n","Iteration:  550, Loss: 519416.812500\n","Iteration:  560, Loss: 519257.750000\n","Iteration:  570, Loss: 519116.531250\n","Iteration:  580, Loss: 518999.187500\n","Iteration:  590, Loss: 518899.125000\n","Iteration:  600, Loss: 518822.187500\n","Elapsed time: 879.39s\n","X is normalized.\n","X is normalized.\n","Found nearest neighbor\n","Found scaled dist\n","(740012, 2)\n","Initial Loss: 1365571.875\n","Iteration:   10, Loss: 676921.000000\n","Iteration:   20, Loss: 320610.000000\n","Iteration:   30, Loss: 240615.671875\n","Iteration:   40, Loss: 218081.531250\n","Iteration:   50, Loss: 210853.890625\n","Iteration:   60, Loss: 207391.828125\n","Iteration:   70, Loss: 206573.156250\n","Iteration:   80, Loss: 206135.937500\n","Iteration:   90, Loss: 206020.890625\n","Iteration:  100, Loss: 205964.218750\n","Iteration:  110, Loss: 310640.218750\n","Iteration:  120, Loss: 309690.218750\n","Iteration:  130, Loss: 309410.218750\n","Iteration:  140, Loss: 309337.781250\n","Iteration:  150, Loss: 309270.281250\n","Iteration:  160, Loss: 309238.406250\n","Iteration:  170, Loss: 309235.500000\n","Iteration:  180, Loss: 309253.718750\n","Iteration:  190, Loss: 309267.812500\n","Iteration:  200, Loss: 309316.531250\n","Iteration:  210, Loss: 103070.023438\n","Iteration:  220, Loss: 102993.859375\n","Iteration:  230, Loss: 102977.296875\n","Iteration:  240, Loss: 102971.562500\n","Iteration:  250, Loss: 102969.875000\n","Iteration:  260, Loss: 102968.937500\n","Iteration:  270, Loss: 102968.835938\n","Iteration:  280, Loss: 102968.804688\n","Iteration:  290, Loss: 102968.929688\n","Iteration:  300, Loss: 102968.921875\n","Iteration:  310, Loss: 102968.828125\n","Iteration:  320, Loss: 102968.750000\n","Iteration:  330, Loss: 102968.765625\n","Iteration:  340, Loss: 102968.750000\n","Iteration:  350, Loss: 102968.812500\n","Iteration:  360, Loss: 102968.773438\n","Iteration:  370, Loss: 102968.789062\n","Iteration:  380, Loss: 102968.812500\n","Iteration:  390, Loss: 102968.820312\n","Iteration:  400, Loss: 102968.812500\n","Iteration:  410, Loss: 102968.804688\n","Iteration:  420, Loss: 102968.812500\n","Iteration:  430, Loss: 102968.820312\n","Iteration:  440, Loss: 102968.820312\n","Iteration:  450, Loss: 102968.812500\n","Iteration:  460, Loss: 102968.820312\n","Iteration:  470, Loss: 102968.812500\n","Iteration:  480, Loss: 102968.804688\n","Iteration:  490, Loss: 102968.804688\n","Iteration:  500, Loss: 102968.812500\n","Iteration:  510, Loss: 102968.812500\n","Iteration:  520, Loss: 102968.812500\n","Iteration:  530, Loss: 102968.812500\n","Iteration:  540, Loss: 102968.804688\n","Iteration:  550, Loss: 102968.812500\n","Iteration:  560, Loss: 102968.804688\n","Iteration:  570, Loss: 102968.804688\n","Iteration:  580, Loss: 102968.804688\n","Iteration:  590, Loss: 102968.796875\n","Iteration:  600, Loss: 102968.804688\n","Elapsed time: 0:00:42.113465\n","X is normalized.\n","X is normalized.\n","Found nearest neighbor\n","Found scaled dist\n","(3705078, 2)\n","Initial Loss: 6933935.0\n","Iteration:   10, Loss: 3396822.250000\n","Iteration:   20, Loss: 1616747.375000\n","Iteration:   30, Loss: 1211790.375000\n","Iteration:   40, Loss: 1088710.875000\n","Iteration:   50, Loss: 1048615.375000\n","Iteration:   60, Loss: 1034977.000000\n","Iteration:   70, Loss: 1032236.312500\n","Iteration:   80, Loss: 1030683.562500\n","Iteration:   90, Loss: 1030327.437500\n","Iteration:  100, Loss: 1030147.500000\n","Iteration:  110, Loss: 1537566.500000\n","Iteration:  120, Loss: 1530272.000000\n","Iteration:  130, Loss: 1528360.375000\n","Iteration:  140, Loss: 1527900.250000\n","Iteration:  150, Loss: 1527582.250000\n","Iteration:  160, Loss: 1527519.125000\n","Iteration:  170, Loss: 1527490.875000\n","Iteration:  180, Loss: 1527584.750000\n","Iteration:  190, Loss: 1527692.000000\n","Iteration:  200, Loss: 1527946.875000\n","Iteration:  210, Loss: 515342.875000\n","Iteration:  220, Loss: 515100.968750\n","Iteration:  230, Loss: 515053.093750\n","Iteration:  240, Loss: 515036.531250\n","Iteration:  250, Loss: 515029.906250\n","Iteration:  260, Loss: 515028.687500\n","Iteration:  270, Loss: 515026.812500\n","Iteration:  280, Loss: 515027.312500\n","Iteration:  290, Loss: 515027.312500\n","Iteration:  300, Loss: 515028.062500\n","Iteration:  310, Loss: 515028.750000\n","Iteration:  320, Loss: 515028.500000\n","Iteration:  330, Loss: 515028.406250\n","Iteration:  340, Loss: 515028.281250\n","Iteration:  350, Loss: 515028.468750\n","Iteration:  360, Loss: 515028.437500\n","Iteration:  370, Loss: 515028.468750\n","Iteration:  380, Loss: 515028.437500\n","Iteration:  390, Loss: 515028.406250\n","Iteration:  400, Loss: 515028.406250\n","Iteration:  410, Loss: 515028.468750\n","Iteration:  420, Loss: 515028.437500\n","Iteration:  430, Loss: 515028.468750\n","Iteration:  440, Loss: 515028.406250\n","Iteration:  450, Loss: 515028.468750\n","Iteration:  460, Loss: 515028.468750\n","Iteration:  470, Loss: 515028.468750\n","Iteration:  480, Loss: 515028.500000\n","Iteration:  490, Loss: 515028.437500\n","Iteration:  500, Loss: 515028.468750\n","Iteration:  510, Loss: 515028.468750\n","Iteration:  520, Loss: 515028.468750\n","Iteration:  530, Loss: 515028.437500\n","Iteration:  540, Loss: 515028.468750\n","Iteration:  550, Loss: 515028.437500\n","Iteration:  560, Loss: 515028.468750\n","Iteration:  570, Loss: 515028.406250\n","Iteration:  580, Loss: 515028.437500\n","Iteration:  590, Loss: 515028.437500\n","Iteration:  600, Loss: 515028.437500\n","Elapsed time: 0:02:48.223025\n","X is normalized\n","PaCMAP(n_neighbors=26, n_MN=13, n_FP=52, distance=euclidean, lr=1.0, n_iters=600, apply_pca=True, opt_method='adam', verbose=True, intermediate=False, seed=None)\n","Finding pairs\n","Found nearest neighbor\n","Calculated sigma\n","Found scaled dist\n","Pairs sampled successfully.\n","((2959892, 2), (1479946, 2), (5919784, 2))\n","Initial Loss: 3658542.0\n","Iteration:   10, Loss: 2462080.750000\n","Iteration:   20, Loss: 2168917.750000\n","Iteration:   30, Loss: 2027364.000000\n","Iteration:   40, Loss: 1920532.750000\n","Iteration:   50, Loss: 1820401.875000\n","Iteration:   60, Loss: 1715139.500000\n","Iteration:   70, Loss: 1597510.250000\n","Iteration:   80, Loss: 1458709.250000\n","Iteration:   90, Loss: 1283450.500000\n","Iteration:  100, Loss: 1020148.000000\n","Iteration:  110, Loss: 1338867.250000\n","Iteration:  120, Loss: 1314390.375000\n","Iteration:  130, Loss: 1302993.125000\n","Iteration:  140, Loss: 1298980.000000\n","Iteration:  150, Loss: 1298095.500000\n","Iteration:  160, Loss: 1298073.750000\n","Iteration:  170, Loss: 1298122.125000\n","Iteration:  180, Loss: 1298818.000000\n","Iteration:  190, Loss: 1299688.750000\n","Iteration:  200, Loss: 1300409.000000\n","Iteration:  210, Loss: 549907.000000\n","Iteration:  220, Loss: 543790.375000\n","Iteration:  230, Loss: 539142.000000\n","Iteration:  240, Loss: 536519.687500\n","Iteration:  250, Loss: 534333.687500\n","Iteration:  260, Loss: 532612.312500\n","Iteration:  270, Loss: 531313.125000\n","Iteration:  280, Loss: 530349.500000\n","Iteration:  290, Loss: 529489.000000\n","Iteration:  300, Loss: 528621.375000\n","Iteration:  310, Loss: 527672.375000\n","Iteration:  320, Loss: 526673.125000\n","Iteration:  330, Loss: 525719.875000\n","Iteration:  340, Loss: 524890.312500\n","Iteration:  350, Loss: 524186.875000\n","Iteration:  360, Loss: 523655.000000\n","Iteration:  370, Loss: 523274.468750\n","Iteration:  380, Loss: 523012.375000\n","Iteration:  390, Loss: 522795.437500\n","Iteration:  400, Loss: 522573.375000\n","Iteration:  410, Loss: 522316.250000\n","Iteration:  420, Loss: 522037.906250\n","Iteration:  430, Loss: 521768.656250\n","Iteration:  440, Loss: 521515.937500\n","Iteration:  450, Loss: 521278.062500\n","Iteration:  460, Loss: 521068.625000\n","Iteration:  470, Loss: 520864.375000\n","Iteration:  480, Loss: 520684.531250\n","Iteration:  490, Loss: 520514.562500\n","Iteration:  500, Loss: 520330.968750\n","Iteration:  510, Loss: 520148.875000\n","Iteration:  520, Loss: 519971.687500\n","Iteration:  530, Loss: 519801.625000\n","Iteration:  540, Loss: 519620.906250\n","Iteration:  550, Loss: 519446.281250\n","Iteration:  560, Loss: 519287.656250\n","Iteration:  570, Loss: 519144.437500\n","Iteration:  580, Loss: 519022.687500\n","Iteration:  590, Loss: 518918.687500\n","Iteration:  600, Loss: 518842.718750\n","Elapsed time: 967.84s\n","X is normalized.\n","X is normalized.\n","Found nearest neighbor\n","Found scaled dist\n","(740012, 2)\n","Initial Loss: 1364858.75\n","Iteration:   10, Loss: 674567.687500\n","Iteration:   20, Loss: 322783.593750\n","Iteration:   30, Loss: 238324.328125\n","Iteration:   40, Loss: 217861.000000\n","Iteration:   50, Loss: 210956.625000\n","Iteration:   60, Loss: 207241.203125\n","Iteration:   70, Loss: 206500.000000\n","Iteration:   80, Loss: 206049.015625\n","Iteration:   90, Loss: 205946.312500\n","Iteration:  100, Loss: 205881.281250\n","Iteration:  110, Loss: 310427.062500\n","Iteration:  120, Loss: 309542.218750\n","Iteration:  130, Loss: 309276.250000\n","Iteration:  140, Loss: 309210.062500\n","Iteration:  150, Loss: 309139.843750\n","Iteration:  160, Loss: 309117.281250\n","Iteration:  170, Loss: 309115.875000\n","Iteration:  180, Loss: 309137.593750\n","Iteration:  190, Loss: 309161.343750\n","Iteration:  200, Loss: 309193.031250\n","Iteration:  210, Loss: 103028.562500\n","Iteration:  220, Loss: 102953.109375\n","Iteration:  230, Loss: 102937.515625\n","Iteration:  240, Loss: 102932.210938\n","Iteration:  250, Loss: 102930.960938\n","Iteration:  260, Loss: 102930.101562\n","Iteration:  270, Loss: 102929.601562\n","Iteration:  280, Loss: 102929.429688\n","Iteration:  290, Loss: 102929.507812\n","Iteration:  300, Loss: 102929.492188\n","Iteration:  310, Loss: 102929.460938\n","Iteration:  320, Loss: 102929.429688\n","Iteration:  330, Loss: 102929.382812\n","Iteration:  340, Loss: 102929.390625\n","Iteration:  350, Loss: 102929.398438\n","Iteration:  360, Loss: 102929.367188\n","Iteration:  370, Loss: 102929.359375\n","Iteration:  380, Loss: 102929.351562\n","Iteration:  390, Loss: 102929.367188\n","Iteration:  400, Loss: 102929.367188\n","Iteration:  410, Loss: 102929.375000\n","Iteration:  420, Loss: 102929.367188\n","Iteration:  430, Loss: 102929.359375\n","Iteration:  440, Loss: 102929.367188\n","Iteration:  450, Loss: 102929.359375\n","Iteration:  460, Loss: 102929.359375\n","Iteration:  470, Loss: 102929.359375\n","Iteration:  480, Loss: 102929.367188\n","Iteration:  490, Loss: 102929.359375\n","Iteration:  500, Loss: 102929.367188\n","Iteration:  510, Loss: 102929.367188\n","Iteration:  520, Loss: 102929.367188\n","Iteration:  530, Loss: 102929.367188\n","Iteration:  540, Loss: 102929.367188\n","Iteration:  550, Loss: 102929.351562\n","Iteration:  560, Loss: 102929.367188\n","Iteration:  570, Loss: 102929.359375\n","Iteration:  580, Loss: 102929.367188\n","Iteration:  590, Loss: 102929.351562\n","Iteration:  600, Loss: 102929.367188\n","Elapsed time: 0:00:41.172776\n","X is normalized.\n","X is normalized.\n","Found nearest neighbor\n","Found scaled dist\n","(3705078, 2)\n","Initial Loss: 6930017.5\n","Iteration:   10, Loss: 3384176.500000\n","Iteration:   20, Loss: 1627611.875000\n","Iteration:   30, Loss: 1199102.500000\n","Iteration:   40, Loss: 1086409.500000\n","Iteration:   50, Loss: 1048741.625000\n","Iteration:   60, Loss: 1034053.937500\n","Iteration:   70, Loss: 1031555.125000\n","Iteration:   80, Loss: 1029993.062500\n","Iteration:   90, Loss: 1029632.562500\n","Iteration:  100, Loss: 1029439.500000\n","Iteration:  110, Loss: 1535855.250000\n","Iteration:  120, Loss: 1528840.625000\n","Iteration:  130, Loss: 1527028.250000\n","Iteration:  140, Loss: 1526684.125000\n","Iteration:  150, Loss: 1526423.500000\n","Iteration:  160, Loss: 1526332.500000\n","Iteration:  170, Loss: 1526322.000000\n","Iteration:  180, Loss: 1526342.500000\n","Iteration:  190, Loss: 1526387.375000\n","Iteration:  200, Loss: 1526557.000000\n","Iteration:  210, Loss: 514994.937500\n","Iteration:  220, Loss: 514758.250000\n","Iteration:  230, Loss: 514708.781250\n","Iteration:  240, Loss: 514692.125000\n","Iteration:  250, Loss: 514687.312500\n","Iteration:  260, Loss: 514684.000000\n","Iteration:  270, Loss: 514683.593750\n","Iteration:  280, Loss: 514683.812500\n","Iteration:  290, Loss: 514684.718750\n","Iteration:  300, Loss: 514683.718750\n","Iteration:  310, Loss: 514683.468750\n","Iteration:  320, Loss: 514683.500000\n","Iteration:  330, Loss: 514683.656250\n","Iteration:  340, Loss: 514683.437500\n","Iteration:  350, Loss: 514683.437500\n","Iteration:  360, Loss: 514683.437500\n","Iteration:  370, Loss: 514683.406250\n","Iteration:  380, Loss: 514683.437500\n","Iteration:  390, Loss: 514683.437500\n","Iteration:  400, Loss: 514683.437500\n","Iteration:  410, Loss: 514683.406250\n","Iteration:  420, Loss: 514683.406250\n","Iteration:  430, Loss: 514683.437500\n","Iteration:  440, Loss: 514683.437500\n","Iteration:  450, Loss: 514683.406250\n","Iteration:  460, Loss: 514683.406250\n","Iteration:  470, Loss: 514683.437500\n","Iteration:  480, Loss: 514683.437500\n","Iteration:  490, Loss: 514683.406250\n","Iteration:  500, Loss: 514683.437500\n","Iteration:  510, Loss: 514683.437500\n","Iteration:  520, Loss: 514683.406250\n","Iteration:  530, Loss: 514683.406250\n","Iteration:  540, Loss: 514683.406250\n","Iteration:  550, Loss: 514683.437500\n","Iteration:  560, Loss: 514683.437500\n","Iteration:  570, Loss: 514683.375000\n","Iteration:  580, Loss: 514683.406250\n","Iteration:  590, Loss: 514683.406250\n","Iteration:  600, Loss: 514683.406250\n","Elapsed time: 0:02:51.090829\n","X is normalized\n","PaCMAP(n_neighbors=26, n_MN=13, n_FP=52, distance=euclidean, lr=1.0, n_iters=600, apply_pca=True, opt_method='adam', verbose=True, intermediate=False, seed=None)\n","Finding pairs\n","Found nearest neighbor\n","Calculated sigma\n","Found scaled dist\n","Pairs sampled successfully.\n","((2959892, 2), (1479946, 2), (5919784, 2))\n","Initial Loss: 3658542.0\n","Iteration:   10, Loss: 2462228.500000\n","Iteration:   20, Loss: 2169084.500000\n","Iteration:   30, Loss: 2027720.000000\n","Iteration:   40, Loss: 1921065.250000\n","Iteration:   50, Loss: 1820951.250000\n","Iteration:   60, Loss: 1715645.750000\n","Iteration:   70, Loss: 1597945.750000\n","Iteration:   80, Loss: 1459024.625000\n","Iteration:   90, Loss: 1283728.250000\n","Iteration:  100, Loss: 1020258.000000\n","Iteration:  110, Loss: 1339194.250000\n","Iteration:  120, Loss: 1314592.500000\n","Iteration:  130, Loss: 1303054.625000\n","Iteration:  140, Loss: 1298984.000000\n","Iteration:  150, Loss: 1298023.875000\n","Iteration:  160, Loss: 1298056.250000\n","Iteration:  170, Loss: 1298145.250000\n","Iteration:  180, Loss: 1299037.500000\n","Iteration:  190, Loss: 1299730.375000\n","Iteration:  200, Loss: 1300308.500000\n","Iteration:  210, Loss: 550008.562500\n","Iteration:  220, Loss: 543931.687500\n","Iteration:  230, Loss: 539251.000000\n","Iteration:  240, Loss: 536633.312500\n","Iteration:  250, Loss: 534473.437500\n","Iteration:  260, Loss: 532723.187500\n","Iteration:  270, Loss: 531406.250000\n","Iteration:  280, Loss: 530420.875000\n","Iteration:  290, Loss: 529577.125000\n","Iteration:  300, Loss: 528682.000000\n","Iteration:  310, Loss: 527744.750000\n","Iteration:  320, Loss: 526739.937500\n","Iteration:  330, Loss: 525800.250000\n","Iteration:  340, Loss: 524975.687500\n","Iteration:  350, Loss: 524286.437500\n","Iteration:  360, Loss: 523761.625000\n","Iteration:  370, Loss: 523366.750000\n","Iteration:  380, Loss: 523102.687500\n","Iteration:  390, Loss: 522893.656250\n","Iteration:  400, Loss: 522681.937500\n","Iteration:  410, Loss: 522424.156250\n","Iteration:  420, Loss: 522150.031250\n","Iteration:  430, Loss: 521882.843750\n","Iteration:  440, Loss: 521626.750000\n","Iteration:  450, Loss: 521384.187500\n","Iteration:  460, Loss: 521169.093750\n","Iteration:  470, Loss: 520965.000000\n","Iteration:  480, Loss: 520789.156250\n","Iteration:  490, Loss: 520619.343750\n","Iteration:  500, Loss: 520441.750000\n","Iteration:  510, Loss: 520258.968750\n","Iteration:  520, Loss: 520090.625000\n","Iteration:  530, Loss: 519918.125000\n","Iteration:  540, Loss: 519737.375000\n","Iteration:  550, Loss: 519561.312500\n","Iteration:  560, Loss: 519397.562500\n","Iteration:  570, Loss: 519260.593750\n","Iteration:  580, Loss: 519141.562500\n","Iteration:  590, Loss: 519041.437500\n","Iteration:  600, Loss: 518958.125000\n","Elapsed time: 1040.94s\n","X is normalized.\n","X is normalized.\n","Found nearest neighbor\n","Found scaled dist\n","(740012, 2)\n","Initial Loss: 1364581.25\n","Iteration:   10, Loss: 674403.562500\n","Iteration:   20, Loss: 322500.750000\n","Iteration:   30, Loss: 238410.531250\n","Iteration:   40, Loss: 217722.359375\n","Iteration:   50, Loss: 210956.562500\n","Iteration:   60, Loss: 207254.578125\n","Iteration:   70, Loss: 206505.906250\n","Iteration:   80, Loss: 206043.531250\n","Iteration:   90, Loss: 205943.093750\n","Iteration:  100, Loss: 205883.437500\n","Iteration:  110, Loss: 310455.875000\n","Iteration:  120, Loss: 309547.406250\n","Iteration:  130, Loss: 309279.718750\n","Iteration:  140, Loss: 309179.625000\n","Iteration:  150, Loss: 309129.312500\n","Iteration:  160, Loss: 309112.031250\n","Iteration:  170, Loss: 309102.437500\n","Iteration:  180, Loss: 309140.187500\n","Iteration:  190, Loss: 309146.593750\n","Iteration:  200, Loss: 309176.125000\n","Iteration:  210, Loss: 103024.945312\n","Iteration:  220, Loss: 102951.804688\n","Iteration:  230, Loss: 102937.000000\n","Iteration:  240, Loss: 102931.187500\n","Iteration:  250, Loss: 102929.265625\n","Iteration:  260, Loss: 102928.445312\n","Iteration:  270, Loss: 102928.445312\n","Iteration:  280, Loss: 102928.195312\n","Iteration:  290, Loss: 102928.117188\n","Iteration:  300, Loss: 102928.195312\n","Iteration:  310, Loss: 102928.093750\n","Iteration:  320, Loss: 102928.210938\n","Iteration:  330, Loss: 102928.257812\n","Iteration:  340, Loss: 102928.210938\n","Iteration:  350, Loss: 102928.257812\n","Iteration:  360, Loss: 102928.218750\n","Iteration:  370, Loss: 102928.242188\n","Iteration:  380, Loss: 102928.234375\n","Iteration:  390, Loss: 102928.226562\n","Iteration:  400, Loss: 102928.242188\n","Iteration:  410, Loss: 102928.226562\n","Iteration:  420, Loss: 102928.242188\n","Iteration:  430, Loss: 102928.242188\n","Iteration:  440, Loss: 102928.242188\n","Iteration:  450, Loss: 102928.234375\n","Iteration:  460, Loss: 102928.234375\n","Iteration:  470, Loss: 102928.234375\n","Iteration:  480, Loss: 102928.234375\n","Iteration:  490, Loss: 102928.234375\n","Iteration:  500, Loss: 102928.234375\n","Iteration:  510, Loss: 102928.234375\n","Iteration:  520, Loss: 102928.234375\n","Iteration:  530, Loss: 102928.226562\n","Iteration:  540, Loss: 102928.242188\n","Iteration:  550, Loss: 102928.234375\n","Iteration:  560, Loss: 102928.226562\n","Iteration:  570, Loss: 102928.234375\n","Iteration:  580, Loss: 102928.242188\n","Iteration:  590, Loss: 102928.242188\n","Iteration:  600, Loss: 102928.242188\n","Elapsed time: 0:00:42.933348\n","X is normalized.\n","X is normalized.\n","Found nearest neighbor\n","Found scaled dist\n","(3705078, 2)\n","Initial Loss: 6927484.5\n","Iteration:   10, Loss: 3382809.750000\n","Iteration:   20, Loss: 1625157.625000\n","Iteration:   30, Loss: 1199445.500000\n","Iteration:   40, Loss: 1085034.125000\n","Iteration:   50, Loss: 1048359.812500\n","Iteration:   60, Loss: 1033685.625000\n","Iteration:   70, Loss: 1031175.625000\n","Iteration:   80, Loss: 1029593.312500\n","Iteration:   90, Loss: 1029247.875000\n","Iteration:  100, Loss: 1029056.437500\n","Iteration:  110, Loss: 1535602.500000\n","Iteration:  120, Loss: 1528295.000000\n","Iteration:  130, Loss: 1526493.000000\n","Iteration:  140, Loss: 1526122.125000\n","Iteration:  150, Loss: 1525808.375000\n","Iteration:  160, Loss: 1525751.375000\n","Iteration:  170, Loss: 1525773.750000\n","Iteration:  180, Loss: 1525901.375000\n","Iteration:  190, Loss: 1525961.375000\n","Iteration:  200, Loss: 1526223.125000\n","Iteration:  210, Loss: 514798.750000\n","Iteration:  220, Loss: 514564.781250\n","Iteration:  230, Loss: 514513.218750\n","Iteration:  240, Loss: 514494.156250\n","Iteration:  250, Loss: 514489.375000\n","Iteration:  260, Loss: 514485.906250\n","Iteration:  270, Loss: 514485.093750\n","Iteration:  280, Loss: 514485.187500\n","Iteration:  290, Loss: 514485.312500\n","Iteration:  300, Loss: 514484.250000\n","Iteration:  310, Loss: 514484.875000\n","Iteration:  320, Loss: 514484.406250\n","Iteration:  330, Loss: 514484.625000\n","Iteration:  340, Loss: 514484.687500\n","Iteration:  350, Loss: 514484.625000\n","Iteration:  360, Loss: 514484.593750\n","Iteration:  370, Loss: 514484.625000\n","Iteration:  380, Loss: 514484.687500\n","Iteration:  390, Loss: 514484.687500\n","Iteration:  400, Loss: 514484.781250\n","Iteration:  410, Loss: 514484.781250\n","Iteration:  420, Loss: 514484.750000\n","Iteration:  430, Loss: 514484.750000\n","Iteration:  440, Loss: 514484.718750\n","Iteration:  450, Loss: 514484.718750\n","Iteration:  460, Loss: 514484.781250\n","Iteration:  470, Loss: 514484.718750\n","Iteration:  480, Loss: 514484.718750\n","Iteration:  490, Loss: 514484.718750\n","Iteration:  500, Loss: 514484.718750\n","Iteration:  510, Loss: 514484.718750\n","Iteration:  520, Loss: 514484.718750\n","Iteration:  530, Loss: 514484.750000\n","Iteration:  540, Loss: 514484.750000\n","Iteration:  550, Loss: 514484.750000\n","Iteration:  560, Loss: 514484.750000\n","Iteration:  570, Loss: 514484.718750\n","Iteration:  580, Loss: 514484.750000\n","Iteration:  590, Loss: 514484.750000\n","Iteration:  600, Loss: 514484.750000\n","Elapsed time: 0:02:45.677295\n","X is normalized\n","PaCMAP(n_neighbors=26, n_MN=13, n_FP=52, distance=euclidean, lr=1.0, n_iters=600, apply_pca=True, opt_method='adam', verbose=True, intermediate=False, seed=None)\n","Finding pairs\n","Found nearest neighbor\n","Calculated sigma\n","Found scaled dist\n","Pairs sampled successfully.\n","((2959892, 2), (1479946, 2), (5919784, 2))\n","Initial Loss: 3658542.0\n","Iteration:   10, Loss: 2462014.750000\n","Iteration:   20, Loss: 2168820.750000\n","Iteration:   30, Loss: 2027136.750000\n","Iteration:   40, Loss: 1920309.000000\n","Iteration:   50, Loss: 1820267.875000\n","Iteration:   60, Loss: 1715031.250000\n","Iteration:   70, Loss: 1597395.125000\n","Iteration:   80, Loss: 1458557.750000\n","Iteration:   90, Loss: 1283323.375000\n","Iteration:  100, Loss: 1020130.875000\n","Iteration:  110, Loss: 1339237.375000\n","Iteration:  120, Loss: 1314506.875000\n","Iteration:  130, Loss: 1302927.750000\n","Iteration:  140, Loss: 1298830.375000\n","Iteration:  150, Loss: 1297889.250000\n","Iteration:  160, Loss: 1297887.000000\n","Iteration:  170, Loss: 1297922.750000\n","Iteration:  180, Loss: 1298658.375000\n","Iteration:  190, Loss: 1299626.875000\n","Iteration:  200, Loss: 1300087.875000\n","Iteration:  210, Loss: 549942.562500\n","Iteration:  220, Loss: 543853.375000\n","Iteration:  230, Loss: 539218.625000\n","Iteration:  240, Loss: 536597.250000\n","Iteration:  250, Loss: 534421.937500\n","Iteration:  260, Loss: 532690.812500\n","Iteration:  270, Loss: 531418.062500\n","Iteration:  280, Loss: 530445.000000\n","Iteration:  290, Loss: 529595.750000\n","Iteration:  300, Loss: 528729.687500\n","Iteration:  310, Loss: 527797.562500\n","Iteration:  320, Loss: 526804.937500\n","Iteration:  330, Loss: 525865.125000\n","Iteration:  340, Loss: 525032.062500\n","Iteration:  350, Loss: 524318.875000\n","Iteration:  360, Loss: 523779.406250\n","Iteration:  370, Loss: 523372.781250\n","Iteration:  380, Loss: 523088.750000\n","Iteration:  390, Loss: 522868.718750\n","Iteration:  400, Loss: 522654.718750\n","Iteration:  410, Loss: 522396.031250\n","Iteration:  420, Loss: 522121.968750\n","Iteration:  430, Loss: 521850.875000\n","Iteration:  440, Loss: 521585.062500\n","Iteration:  450, Loss: 521340.375000\n","Iteration:  460, Loss: 521126.406250\n","Iteration:  470, Loss: 520925.562500\n","Iteration:  480, Loss: 520745.656250\n","Iteration:  490, Loss: 520569.687500\n","Iteration:  500, Loss: 520394.625000\n","Iteration:  510, Loss: 520212.937500\n","Iteration:  520, Loss: 520038.875000\n","Iteration:  530, Loss: 519867.218750\n","Iteration:  540, Loss: 519683.687500\n","Iteration:  550, Loss: 519506.750000\n","Iteration:  560, Loss: 519348.437500\n","Iteration:  570, Loss: 519203.000000\n","Iteration:  580, Loss: 519081.375000\n","Iteration:  590, Loss: 518977.437500\n","Iteration:  600, Loss: 518896.718750\n","Elapsed time: 912.02s\n","X is normalized.\n","X is normalized.\n","Found nearest neighbor\n","Found scaled dist\n","(740012, 2)\n","Initial Loss: 1364775.5\n","Iteration:   10, Loss: 677228.125000\n","Iteration:   20, Loss: 324052.687500\n","Iteration:   30, Loss: 237712.921875\n","Iteration:   40, Loss: 217740.171875\n","Iteration:   50, Loss: 211023.531250\n","Iteration:   60, Loss: 207254.812500\n","Iteration:   70, Loss: 206521.921875\n","Iteration:   80, Loss: 206056.359375\n","Iteration:   90, Loss: 205958.734375\n","Iteration:  100, Loss: 205895.484375\n","Iteration:  110, Loss: 310511.062500\n","Iteration:  120, Loss: 309583.812500\n","Iteration:  130, Loss: 309306.531250\n","Iteration:  140, Loss: 309206.906250\n","Iteration:  150, Loss: 309151.875000\n","Iteration:  160, Loss: 309115.093750\n","Iteration:  170, Loss: 309116.812500\n","Iteration:  180, Loss: 309143.218750\n","Iteration:  190, Loss: 309161.281250\n","Iteration:  200, Loss: 309207.156250\n","Iteration:  210, Loss: 103036.289062\n","Iteration:  220, Loss: 102959.578125\n","Iteration:  230, Loss: 102944.429688\n","Iteration:  240, Loss: 102938.273438\n","Iteration:  250, Loss: 102936.968750\n","Iteration:  260, Loss: 102936.507812\n","Iteration:  270, Loss: 102935.921875\n","Iteration:  280, Loss: 102935.937500\n","Iteration:  290, Loss: 102935.914062\n","Iteration:  300, Loss: 102935.937500\n","Iteration:  310, Loss: 102935.835938\n","Iteration:  320, Loss: 102935.929688\n","Iteration:  330, Loss: 102935.890625\n","Iteration:  340, Loss: 102935.875000\n","Iteration:  350, Loss: 102935.843750\n","Iteration:  360, Loss: 102935.835938\n","Iteration:  370, Loss: 102935.835938\n","Iteration:  380, Loss: 102935.820312\n","Iteration:  390, Loss: 102935.820312\n","Iteration:  400, Loss: 102935.804688\n","Iteration:  410, Loss: 102935.820312\n","Iteration:  420, Loss: 102935.828125\n","Iteration:  430, Loss: 102935.820312\n","Iteration:  440, Loss: 102935.812500\n","Iteration:  450, Loss: 102935.828125\n","Iteration:  460, Loss: 102935.820312\n","Iteration:  470, Loss: 102935.828125\n","Iteration:  480, Loss: 102935.820312\n","Iteration:  490, Loss: 102935.828125\n","Iteration:  500, Loss: 102935.828125\n","Iteration:  510, Loss: 102935.812500\n","Iteration:  520, Loss: 102935.820312\n","Iteration:  530, Loss: 102935.812500\n","Iteration:  540, Loss: 102935.812500\n","Iteration:  550, Loss: 102935.820312\n","Iteration:  560, Loss: 102935.820312\n","Iteration:  570, Loss: 102935.820312\n","Iteration:  580, Loss: 102935.820312\n","Iteration:  590, Loss: 102935.812500\n","Iteration:  600, Loss: 102935.812500\n","Elapsed time: 0:00:39.042274\n","X is normalized.\n","X is normalized.\n","Found nearest neighbor\n","Found scaled dist\n","(3705078, 2)\n","Initial Loss: 6927528.5\n","Iteration:   10, Loss: 3391674.250000\n","Iteration:   20, Loss: 1631289.625000\n","Iteration:   30, Loss: 1195200.750000\n","Iteration:   40, Loss: 1085061.625000\n","Iteration:   50, Loss: 1048482.187500\n","Iteration:   60, Loss: 1033541.750000\n","Iteration:   70, Loss: 1031082.187500\n","Iteration:   80, Loss: 1029503.437500\n","Iteration:   90, Loss: 1029159.062500\n","Iteration:  100, Loss: 1028969.937500\n","Iteration:  110, Loss: 1535430.125000\n","Iteration:  120, Loss: 1528348.000000\n","Iteration:  130, Loss: 1526472.500000\n","Iteration:  140, Loss: 1525935.250000\n","Iteration:  150, Loss: 1525612.625000\n","Iteration:  160, Loss: 1525484.375000\n","Iteration:  170, Loss: 1525469.000000\n","Iteration:  180, Loss: 1525584.625000\n","Iteration:  190, Loss: 1525725.250000\n","Iteration:  200, Loss: 1526011.000000\n","Iteration:  210, Loss: 514752.437500\n","Iteration:  220, Loss: 514520.218750\n","Iteration:  230, Loss: 514470.781250\n","Iteration:  240, Loss: 514454.062500\n","Iteration:  250, Loss: 514447.750000\n","Iteration:  260, Loss: 514444.875000\n","Iteration:  270, Loss: 514444.531250\n","Iteration:  280, Loss: 514445.375000\n","Iteration:  290, Loss: 514445.125000\n","Iteration:  300, Loss: 514444.843750\n","Iteration:  310, Loss: 514445.125000\n","Iteration:  320, Loss: 514445.343750\n","Iteration:  330, Loss: 514445.218750\n","Iteration:  340, Loss: 514445.312500\n","Iteration:  350, Loss: 514445.500000\n","Iteration:  360, Loss: 514445.562500\n","Iteration:  370, Loss: 514445.531250\n","Iteration:  380, Loss: 514445.500000\n","Iteration:  390, Loss: 514445.406250\n","Iteration:  400, Loss: 514445.500000\n","Iteration:  410, Loss: 514445.531250\n","Iteration:  420, Loss: 514445.562500\n","Iteration:  430, Loss: 514445.562500\n","Iteration:  440, Loss: 514445.531250\n","Iteration:  450, Loss: 514445.500000\n","Iteration:  460, Loss: 514445.468750\n","Iteration:  470, Loss: 514445.500000\n","Iteration:  480, Loss: 514445.500000\n","Iteration:  490, Loss: 514445.531250\n","Iteration:  500, Loss: 514445.531250\n","Iteration:  510, Loss: 514445.531250\n","Iteration:  520, Loss: 514445.562500\n","Iteration:  530, Loss: 514445.531250\n","Iteration:  540, Loss: 514445.531250\n","Iteration:  550, Loss: 514445.531250\n","Iteration:  560, Loss: 514445.531250\n","Iteration:  570, Loss: 514445.500000\n","Iteration:  580, Loss: 514445.500000\n","Iteration:  590, Loss: 514445.531250\n","Iteration:  600, Loss: 514445.500000\n","Elapsed time: 0:02:44.809268\n","X is normalized\n","PaCMAP(n_neighbors=26, n_MN=13, n_FP=52, distance=euclidean, lr=1.0, n_iters=600, apply_pca=True, opt_method='adam', verbose=True, intermediate=False, seed=None)\n","Finding pairs\n","Found nearest neighbor\n","Calculated sigma\n","Found scaled dist\n","Pairs sampled successfully.\n","((2959892, 2), (1479946, 2), (5919784, 2))\n","Initial Loss: 3658542.0\n","Iteration:   10, Loss: 2461168.750000\n","Iteration:   20, Loss: 2167964.500000\n","Iteration:   30, Loss: 2026968.875000\n","Iteration:   40, Loss: 1920261.625000\n","Iteration:   50, Loss: 1820248.375000\n","Iteration:   60, Loss: 1714979.000000\n","Iteration:   70, Loss: 1597390.250000\n","Iteration:   80, Loss: 1458536.625000\n","Iteration:   90, Loss: 1283273.125000\n","Iteration:  100, Loss: 1019990.250000\n","Iteration:  110, Loss: 1338907.375000\n","Iteration:  120, Loss: 1314317.625000\n","Iteration:  130, Loss: 1302762.375000\n","Iteration:  140, Loss: 1298558.625000\n","Iteration:  150, Loss: 1297705.250000\n","Iteration:  160, Loss: 1297648.000000\n","Iteration:  170, Loss: 1297662.875000\n","Iteration:  180, Loss: 1298480.750000\n","Iteration:  190, Loss: 1299458.500000\n","Iteration:  200, Loss: 1299930.875000\n","Iteration:  210, Loss: 549804.687500\n","Iteration:  220, Loss: 543689.000000\n","Iteration:  230, Loss: 539098.625000\n","Iteration:  240, Loss: 536437.125000\n","Iteration:  250, Loss: 534274.312500\n","Iteration:  260, Loss: 532529.750000\n","Iteration:  270, Loss: 531233.125000\n","Iteration:  280, Loss: 530256.250000\n","Iteration:  290, Loss: 529440.062500\n","Iteration:  300, Loss: 528574.750000\n","Iteration:  310, Loss: 527638.437500\n","Iteration:  320, Loss: 526615.687500\n","Iteration:  330, Loss: 525666.250000\n","Iteration:  340, Loss: 524838.062500\n","Iteration:  350, Loss: 524138.875000\n","Iteration:  360, Loss: 523605.562500\n","Iteration:  370, Loss: 523206.375000\n","Iteration:  380, Loss: 522946.093750\n","Iteration:  390, Loss: 522728.531250\n","Iteration:  400, Loss: 522504.875000\n","Iteration:  410, Loss: 522235.656250\n","Iteration:  420, Loss: 521957.062500\n","Iteration:  430, Loss: 521686.375000\n","Iteration:  440, Loss: 521424.843750\n","Iteration:  450, Loss: 521180.187500\n","Iteration:  460, Loss: 520964.062500\n","Iteration:  470, Loss: 520764.562500\n","Iteration:  480, Loss: 520583.156250\n","Iteration:  490, Loss: 520409.218750\n","Iteration:  500, Loss: 520230.125000\n","Iteration:  510, Loss: 520045.812500\n","Iteration:  520, Loss: 519868.000000\n","Iteration:  530, Loss: 519688.406250\n","Iteration:  540, Loss: 519500.406250\n","Iteration:  550, Loss: 519324.062500\n","Iteration:  560, Loss: 519164.937500\n","Iteration:  570, Loss: 519024.562500\n","Iteration:  580, Loss: 518905.218750\n","Iteration:  590, Loss: 518796.468750\n","Iteration:  600, Loss: 518713.218750\n","Elapsed time: 1003.44s\n","X is normalized.\n","X is normalized.\n","Found nearest neighbor\n","Found scaled dist\n","(740012, 2)\n","Initial Loss: 1364907.875\n","Iteration:   10, Loss: 678080.437500\n","Iteration:   20, Loss: 326189.218750\n","Iteration:   30, Loss: 238496.531250\n","Iteration:   40, Loss: 217717.515625\n","Iteration:   50, Loss: 210882.406250\n","Iteration:   60, Loss: 207166.828125\n","Iteration:   70, Loss: 206405.093750\n","Iteration:   80, Loss: 205947.906250\n","Iteration:   90, Loss: 205845.015625\n","Iteration:  100, Loss: 205780.234375\n","Iteration:  110, Loss: 310227.750000\n","Iteration:  120, Loss: 309372.187500\n","Iteration:  130, Loss: 309119.406250\n","Iteration:  140, Loss: 309057.750000\n","Iteration:  150, Loss: 309005.937500\n","Iteration:  160, Loss: 308980.750000\n","Iteration:  170, Loss: 308988.218750\n","Iteration:  180, Loss: 308998.218750\n","Iteration:  190, Loss: 309007.343750\n","Iteration:  200, Loss: 309044.531250\n","Iteration:  210, Loss: 102980.460938\n","Iteration:  220, Loss: 102903.039062\n","Iteration:  230, Loss: 102887.609375\n","Iteration:  240, Loss: 102880.679688\n","Iteration:  250, Loss: 102878.812500\n","Iteration:  260, Loss: 102878.234375\n","Iteration:  270, Loss: 102878.140625\n","Iteration:  280, Loss: 102878.101562\n","Iteration:  290, Loss: 102878.132812\n","Iteration:  300, Loss: 102878.203125\n","Iteration:  310, Loss: 102878.101562\n","Iteration:  320, Loss: 102878.101562\n","Iteration:  330, Loss: 102878.109375\n","Iteration:  340, Loss: 102878.078125\n","Iteration:  350, Loss: 102878.078125\n","Iteration:  360, Loss: 102878.109375\n","Iteration:  370, Loss: 102878.085938\n","Iteration:  380, Loss: 102878.062500\n","Iteration:  390, Loss: 102878.078125\n","Iteration:  400, Loss: 102878.070312\n","Iteration:  410, Loss: 102878.070312\n","Iteration:  420, Loss: 102878.070312\n","Iteration:  430, Loss: 102878.070312\n","Iteration:  440, Loss: 102878.070312\n","Iteration:  450, Loss: 102878.070312\n","Iteration:  460, Loss: 102878.070312\n","Iteration:  470, Loss: 102878.070312\n","Iteration:  480, Loss: 102878.070312\n","Iteration:  490, Loss: 102878.070312\n","Iteration:  500, Loss: 102878.070312\n","Iteration:  510, Loss: 102878.070312\n","Iteration:  520, Loss: 102878.070312\n","Iteration:  530, Loss: 102878.070312\n","Iteration:  540, Loss: 102878.070312\n","Iteration:  550, Loss: 102878.070312\n","Iteration:  560, Loss: 102878.070312\n","Iteration:  570, Loss: 102878.070312\n","Iteration:  580, Loss: 102878.070312\n","Iteration:  590, Loss: 102878.070312\n","Iteration:  600, Loss: 102878.070312\n","Elapsed time: 0:00:40.418348\n","X is normalized.\n","X is normalized.\n","Found nearest neighbor\n","Found scaled dist\n","(3705078, 2)\n","Initial Loss: 6925993.5\n","Iteration:   10, Loss: 3400134.750000\n","Iteration:   20, Loss: 1644751.750000\n","Iteration:   30, Loss: 1199704.625000\n","Iteration:   40, Loss: 1085342.750000\n","Iteration:   50, Loss: 1048856.250000\n","Iteration:   60, Loss: 1033638.562500\n","Iteration:   70, Loss: 1031140.000000\n","Iteration:   80, Loss: 1029530.437500\n","Iteration:   90, Loss: 1029156.375000\n","Iteration:  100, Loss: 1028976.000000\n","Iteration:  110, Loss: 1534681.500000\n","Iteration:  120, Loss: 1527938.375000\n","Iteration:  130, Loss: 1526173.875000\n","Iteration:  140, Loss: 1525939.500000\n","Iteration:  150, Loss: 1525726.500000\n","Iteration:  160, Loss: 1525617.625000\n","Iteration:  170, Loss: 1525703.750000\n","Iteration:  180, Loss: 1525777.625000\n","Iteration:  190, Loss: 1525879.000000\n","Iteration:  200, Loss: 1526003.750000\n","Iteration:  210, Loss: 514762.156250\n","Iteration:  220, Loss: 514522.375000\n","Iteration:  230, Loss: 514473.812500\n","Iteration:  240, Loss: 514451.843750\n","Iteration:  250, Loss: 514446.062500\n","Iteration:  260, Loss: 514444.750000\n","Iteration:  270, Loss: 514444.531250\n","Iteration:  280, Loss: 514443.468750\n","Iteration:  290, Loss: 514442.406250\n","Iteration:  300, Loss: 514442.875000\n","Iteration:  310, Loss: 514442.875000\n","Iteration:  320, Loss: 514442.843750\n","Iteration:  330, Loss: 514442.500000\n","Iteration:  340, Loss: 514442.625000\n","Iteration:  350, Loss: 514442.750000\n","Iteration:  360, Loss: 514442.906250\n","Iteration:  370, Loss: 514442.718750\n","Iteration:  380, Loss: 514442.781250\n","Iteration:  390, Loss: 514442.812500\n","Iteration:  400, Loss: 514442.781250\n","Iteration:  410, Loss: 514442.718750\n","Iteration:  420, Loss: 514442.781250\n","Iteration:  430, Loss: 514442.781250\n","Iteration:  440, Loss: 514442.750000\n","Iteration:  450, Loss: 514442.781250\n","Iteration:  460, Loss: 514442.750000\n","Iteration:  470, Loss: 514442.750000\n","Iteration:  480, Loss: 514442.781250\n","Iteration:  490, Loss: 514442.781250\n","Iteration:  500, Loss: 514442.750000\n","Iteration:  510, Loss: 514442.750000\n","Iteration:  520, Loss: 514442.750000\n","Iteration:  530, Loss: 514442.781250\n","Iteration:  540, Loss: 514442.781250\n","Iteration:  550, Loss: 514442.718750\n","Iteration:  560, Loss: 514442.718750\n","Iteration:  570, Loss: 514442.750000\n","Iteration:  580, Loss: 514442.750000\n","Iteration:  590, Loss: 514442.781250\n","Iteration:  600, Loss: 514442.718750\n","Elapsed time: 0:02:46.108799\n"]}]},{"cell_type":"code","source":["## pacmap과 isolation forest 3차 이용(2)\n","val_score_3 = f1_score(ori_val_df['Class'], np.round(val_pred_set_3), average='macro')\n","\n","print(f'Validation F1 Score : [{val_score_3}]')\n","print(classification_report(ori_val_df['Class'], np.round(val_pred_set_3)))\n","print(confusion_matrix(ori_val_df['Class'], np.round(val_pred_set_3)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P_pRa36E9BIb","executionInfo":{"status":"ok","timestamp":1660136021512,"user_tz":-540,"elapsed":637,"user":{"displayName":"­신기수","userId":"10948147504762254674"}},"outputId":"62bb1b9d-2195-42f3-afe8-1ebb08f7a5db"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation F1 Score : [0.9309641419574388]\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     28432\n","           1       0.89      0.83      0.86        30\n","\n","    accuracy                           1.00     28462\n","   macro avg       0.95      0.92      0.93     28462\n","weighted avg       1.00      1.00      1.00     28462\n","\n","[[28429     3]\n"," [    5    25]]\n"]}]},{"cell_type":"code","source":["set(test_pred_set_3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dNRPSgnt01UH","executionInfo":{"status":"ok","timestamp":1660136159903,"user_tz":-540,"elapsed":284,"user":{"displayName":"­신기수","userId":"10948147504762254674"}},"outputId":"5715d799-c356-4b45-fcb7-1c4758cf15d3"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0.0, 1.0}"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["## pacmap과 isolation forest 3차 이용(3)\n","# 2차 저장\n","chujung_train_2 = pd.DataFrame({'Class':np.round(train_pred_set_3)})\n","chujung_val_2 = pd.DataFrame({'Class':np.round(val_pred_set_3)})\n","chujung_test_2 = pd.DataFrame({'Class':np.round(test_pred_set_3)})\n","\n","result_train_2 = pd.concat([train_df,chujung_train_2], axis=1)\n","result_val_2 = pd.concat([val_df,chujung_val_2], axis=1)\n","result_test_2 = pd.concat([test_df,chujung_test_2], axis=1)\n","\n","result_train_2.to_csv('result_train_2.csv', index=False)\n","result_val_2.to_csv('result_val_2.csv', index=False)\n","result_test_2.to_csv('result_test_2.csv', index=False)"],"metadata":{"id":"_h8Q3KX3FINe","executionInfo":{"status":"ok","timestamp":1660136056721,"user_tz":-540,"elapsed":12198,"user":{"displayName":"­신기수","userId":"10948147504762254674"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["## pacmap과 isolation forest 3차 이용(4)\n","# 2차 불러오기 및 outlier된 dataset 모음\n","train_pred_set_3 = np.array(pd.read_csv('first_result_train.csv')['Class'])\n","val_pred_set_3 = np.array(pd.read_csv('first_result_val.csv')['Class'])\n","test_pred_set_3 = np.array(pd.read_csv('first_result_test.csv')['Class'])\n","\n","one_train_df = train_df.iloc[np.where(np.round(train_pred_set_3) == 1)[0]]\n","one_val_df = ori_val_df.iloc[np.where(np.round(val_pred_set_3) == 1)[0]]\n","one_test_df = test_df.iloc[np.where(np.round(test_pred_set_3) == 1)[0]]"],"metadata":{"id":"jRvSPekcWYhu","executionInfo":{"status":"ok","timestamp":1660136416250,"user_tz":-540,"elapsed":9204,"user":{"displayName":"­신기수","userId":"10948147504762254674"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["저는 이에 더해 판별된 outlier들 중에 가짜를 도출하기로 생각했습니다.\n","역시 저는 변수를 선택하였고 validation dataset의 통계정보를 이용하였습니다. 이번엔 다른 방식으로 판별하고자 합니다. 개인적으로 저는 isolation forest를 사용함에도 표출이 안 된것은 특정 변수하에서는 가짜 outlier들이 진짜 outlier안에 숨어있다고 생각하였습니다. 그에 따라 KernelPCA를 이용해서 안쪽에 있는 가짜 outlier을 빼내고, pacmap을 사용한 다음, pca를 통해 좌표를 바꾸고, 극단값 중 몇몇개를 뽑아서 가짜 outlier라고 생각하였습니다. 우선, Validation dataset 내 ouliter로 판별된 것 중에서 실제 outlier들의 중위값과 가짜 outlier들의 중위값 검정 비교하였고, Validation dataset 내 ouliter로 판별된 것 중에서 실제 outlier들의 분산와 가짜 outlier 비율 체크해서 변수들을 확인하였습니다."],"metadata":{"id":"C7kmXNKH7EuC"}},{"cell_type":"code","source":["## outlier 중 가짜 판별(1)\n","# 변수 선택\n","\n","rrr_df = []\n","for what_val in range(30):\n","    rrr_df.append(ranksums(one_val_df.iloc[np.where(one_val_df['Class'] == 1)[0],what_val],one_val_df.iloc[np.where(one_val_df['Class'] == 0)[0],what_val]).pvalue)\n","\n","rrr_idx = np.where(np.array(rrr_df) > 0.05)[0]\n","\n","qqq_df = []\n","for what_val in rrr_idx:\n","    qqq_df.append(one_val_df.iloc[np.where(one_val_df['Class'] == 0)[0],what_val].var()/one_val_df.iloc[np.where(one_val_df['Class'] == 1)[0],what_val].var())\n","\n","new_born_idx = np.argsort(qqq_df)[:5]"],"metadata":{"id":"rzRd6lZ3FOvF","executionInfo":{"status":"ok","timestamp":1660136416252,"user_tz":-540,"elapsed":17,"user":{"displayName":"­신기수","userId":"10948147504762254674"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["print(new_born_idx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxgqFL_Q3UXT","executionInfo":{"status":"ok","timestamp":1660136618769,"user_tz":-540,"elapsed":344,"user":{"displayName":"­신기수","userId":"10948147504762254674"}},"outputId":"10c19e4c-c72e-4f95-c005-3e78ff8aaff1"},"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([20, 23, 14,  3, 18])"]},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","source":["다음으로 가짜 판별하는데 있어서 위에서 이야기한 그대로 바탕으로 진행하였습니다. 가짜 outlier인지 아닌지 판별하는데 있어서 Validation set의 통계정보와 wilcoxon rank sum test를 이용하였습니다. "],"metadata":{"id":"18Ryb5lq7bYB"}},{"cell_type":"code","source":["## outlier 중 가짜 판별(2)\n","# 판별\n","transformer = KernelPCA(n_components=5, kernel='rbf')\n","hhhhh = new_born_idx\n","can_sepearate = np.argsort(rrr_df)[0]\n","\n","X_transformed = transformer.fit_transform(one_train_df.iloc[:,hhhhh])\n","X1_transformed = transformer.transform(one_val_df.iloc[:,hhhhh])\n","X2_transformed = transformer.transform(one_test_df.iloc[:,hhhhh])\n","\n","# kernel pca 후 1에서 0 찾기 by pacmap 그후 판별\n","hhmm = 201\n","skip_count = 0\n","\n","contamin = 0.0725\n","making_set = 0\n","\n","for num in range(hhmm):\n","    embedding1010 = pacmap.PaCMAP(n_components=3, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, num_iters = 1000, verbose = False)\n","    pacmac_train1010 = embedding1010.fit_transform(X_transformed, init=\"pca\")\n","    pacmac_val1010 = embedding1010.transform(X1_transformed, basis=X_transformed)\n","    pacmac_test1010 = embedding1010.transform(X2_transformed, basis=X_transformed)\n","\n","    # 그냥 pca\n","    pcawow = PCA(n_components = 3) # 주성분을 몇개로 할지 결정\n","    pca_train_wow = pcawow.fit_transform(pacmac_train1010)\n","    pca_train_wow = pd.DataFrame(data=pca_train_wow)\n","\n","    pca_val_wow = pcawow.transform(pacmac_val1010)\n","    pca_val_wow = pd.DataFrame(data=pca_val_wow)\n","\n","    pca_test_wow = pcawow.transform(pacmac_test1010)\n","    pca_test_wow = pd.DataFrame(data=pca_test_wow)\n","\n","    rlwns = int(contamin*len(pca_train_wow))\n","    thtn = contamin*len(pca_train_wow) - rlwns\n","    \n","    if making_set == 0:\n","        max_part_zero = 0\n","        max_part_one = 0\n","        max_what = 100\n","        \n","        # PC 축을 설정하고 좌 또는 우 극단값을 찾고 해당하는 중위값이 실제 outlier들의 중위값인지 체크\n","        for jrj in range(6):\n","            val_pred_set_1010 = pd.DataFrame({'Class':np.repeat(1,len(pca_val_wow))})       \n","\n","            if jrj % 2 == 0:                \n","                rlwns_val = pca_train_wow.iloc[:,(jrj//2)].nlargest(rlwns).iloc[(rlwns-1)]\n","                rlwns_val_Qkd = pca_train_wow.iloc[:,(jrj//2)].nlargest((rlwns+1)).iloc[(rlwns)]\n","\n","                val_pred_set_1010.iloc[np.where(pca_val_wow.iloc[:,(jrj//2)] >= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn) ))] = 0\n","            else:\n","                rlwns_val = pca_train_wow.iloc[:,(jrj//2)].nsmallest(rlwns).iloc[(rlwns-1)]\n","                rlwns_val_Qkd = pca_train_wow.iloc[:,(jrj//2)].nsmallest((rlwns+1)).iloc[(rlwns)]\n","\n","                val_pred_set_1010.iloc[np.where(pca_val_wow.iloc[:,(jrj//2)] <= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n","\n","            ranksum_pval_ed_one = ranksums(one_val_df.iloc[np.where(one_val_df['Class'] == 1)[0],can_sepearate],one_val_df.iloc[np.where(val_pred_set_1010['Class'] == 1)[0],can_sepearate]).pvalue\n","            ranksum_pval_ed_zero = ranksums(one_val_df.iloc[np.where(one_val_df['Class'] == 0)[0],can_sepearate],one_val_df.iloc[np.where(val_pred_set_1010['Class'] == 0)[0],can_sepearate]).pvalue\n","\n","            if np.isnan(ranksum_pval_ed_zero):\n","                continue\n","            \n","            if max_part_zero <= ranksum_pval_ed_zero:\n","                max_part_zero = ranksum_pval_ed_zero\n","            if max_part_one <= ranksum_pval_ed_one:\n","                max_part_one = ranksum_pval_ed_one\n","\n","            if max_part_zero >= 0.5:\n","                if max_part_one >= 0.5:\n","                    max_what = jrj\n","                    break\n","\n","        if max_what == 100:\n","            skip_count += 1\n","            continue\n","        making_set += 10\n","        \n","        # 체크 후 해당하는 축으로 가짜 outlier 판별\n","        val_pred_set_1010 = pd.DataFrame({'Class':np.repeat(1,len(pca_val_wow))})\n","\n","        if max_what % 2 == 0:\n","            rlwns_val = pca_train_wow.iloc[:,(max_what//2)].nlargest(rlwns).iloc[(rlwns-1)]\n","            rlwns_val_Qkd = pca_train_wow.iloc[:,(max_what//2)].nlargest((rlwns+1)).iloc[(rlwns)]\n","\n","            val_pred_set_1010.iloc[np.where(pca_val_wow.iloc[:,(max_what//2)] >= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n","\n","        else:\n","            rlwns_val = pca_train_wow.iloc[:,(max_what//2)].nsmallest(rlwns).iloc[(rlwns-1)]\n","            rlwns_val_Qkd = pca_train_wow.iloc[:,(max_what//2)].nsmallest((rlwns+1)).iloc[(rlwns)]\n","\n","            val_pred_set_1010.iloc[np.where(pca_val_wow.iloc[:,(max_what//2)] <= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n","        \n","        train_pred_set_1010 = pd.DataFrame({'Class':np.repeat(1,len(pca_train_wow))})\n","\n","        if max_what % 2 == 0:\n","            train_pred_set_1010.iloc[np.where(pca_train_wow.iloc[:,(max_what//2)] >= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n","        else:\n","            train_pred_set_1010.iloc[np.where(pca_train_wow.iloc[:,(max_what//2)] <= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0   \n","\n","        test_pred_set_1010 = pd.DataFrame({'Class':np.repeat(1,len(pca_test_wow))})\n","\n","        if max_what % 2 == 0:\n","            test_pred_set_1010.iloc[np.where(pca_test_wow.iloc[:,(max_what//2)] >= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n","        else:\n","            test_pred_set_1010.iloc[np.where(pca_test_wow.iloc[:,(max_what//2)] <= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0 \n","\n","    else:\n","        max_part_zero = 0\n","        max_part_one = 0\n","        max_what = 100\n","        \n","        # PC 축을 설정하고 좌 또는 우 극단값을 찾고 해당하는 중위값이 실제 outlier들의 중위값인지 체크\n","        for jrj in range(6):\n","            val_pred_1010 = pd.DataFrame({'Class':np.repeat(1,len(pca_val_wow))})\n","\n","            if jrj % 2 == 0:\n","                rlwns_val = pca_train_wow.iloc[:,(jrj//2)].nlargest(rlwns).iloc[(rlwns-1)]\n","                rlwns_val_Qkd = pca_train_wow.iloc[:,(jrj//2)].nlargest((rlwns+1)).iloc[(rlwns)]\n","\n","                val_pred_1010.iloc[np.where(pca_val_wow.iloc[:,(jrj//2)] >= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn) ))] = 0\n","            else:\n","                rlwns_val = pca_train_wow.iloc[:,(jrj//2)].nsmallest(rlwns).iloc[(rlwns-1)]\n","                rlwns_val_Qkd = pca_train_wow.iloc[:,(jrj//2)].nsmallest((rlwns+1)).iloc[(rlwns)]\n","\n","                val_pred_1010.iloc[np.where(pca_val_wow.iloc[:,(jrj//2)] <= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n","\n","            ranksum_pval_ed_one = ranksums(one_val_df.iloc[np.where(one_val_df['Class'] == 1)[0],can_sepearate],one_val_df.iloc[np.where(val_pred_1010['Class'] == 1)[0],can_sepearate]).pvalue\n","            ranksum_pval_ed_zero = ranksums(one_val_df.iloc[np.where(one_val_df['Class'] == 0)[0],can_sepearate],one_val_df.iloc[np.where(val_pred_1010['Class'] == 0)[0],can_sepearate]).pvalue\n","\n","            if np.isnan(ranksum_pval_ed_zero):\n","                continue\n","            \n","            if max_part_zero <= ranksum_pval_ed_zero:\n","                max_part_zero = ranksum_pval_ed_zero\n","            if max_part_one <= ranksum_pval_ed_one:\n","                max_part_one = ranksum_pval_ed_one\n","\n","            if max_part_zero >= 0.5:\n","                if max_part_one >= 0.5:\n","                    max_what = jrj\n","                    break\n","        \n","        if max_what == 100:\n","            skip_count += 1\n","            continue\n","        \n","        # 체크 후 해당하는 축으로 가짜 outlier 판별\n","        val_pred_1010 = pd.DataFrame({'Class':np.repeat(1,len(pca_val_wow))})\n","\n","        if max_what % 2 == 0:\n","            rlwns_val = pca_train_wow.iloc[:,(max_what//2)].nlargest(rlwns).iloc[(rlwns-1)]\n","            rlwns_val_Qkd = pca_train_wow.iloc[:,(max_what//2)].nlargest((rlwns+1)).iloc[(rlwns)]\n","\n","            val_pred_1010.iloc[np.where(pca_val_wow.iloc[:,(max_what//2)] >= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n","        else:\n","            rlwns_val = pca_train_wow.iloc[:,(max_what//2)].nsmallest(rlwns).iloc[(rlwns-1)]\n","            rlwns_val_Qkd = pca_train_wow.iloc[:,(max_what//2)].nsmallest((rlwns+1)).iloc[(rlwns)]\n","\n","            val_pred_1010.iloc[np.where(pca_val_wow.iloc[:,(max_what//2)] <= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n","\n","        ranksum_pval_ed_one = ranksums(one_val_df.iloc[np.where(one_val_df['Class'] == 1)[0],can_sepearate],one_val_df.iloc[np.where(val_pred_1010['Class'] == 1)[0],can_sepearate]).pvalue\n","        ranksum_pval_ed_zero = ranksums(one_val_df.iloc[np.where(one_val_df['Class'] == 0)[0],can_sepearate],one_val_df.iloc[np.where(val_pred_1010['Class'] == 0)[0],can_sepearate]).pvalue\n","\n","        val_pred_set_1010 = val_pred_set_1010 + val_pred_1010\n","\n","        train_pred_1010 = pd.DataFrame({'Class':np.repeat(1,len(pca_train_wow))})\n","\n","        if max_what % 2 == 0:\n","            train_pred_1010.iloc[np.where(pca_train_wow.iloc[:,(max_what//2)] >= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n","        else:\n","            train_pred_1010.iloc[np.where(pca_train_wow.iloc[:,(max_what//2)] <= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0   \n","\n","        train_pred_set_1010 = train_pred_set_1010 + train_pred_1010\n","\n","        test_pred_1010 = pd.DataFrame({'Class':np.repeat(1,len(pca_test_wow))})\n","\n","        if max_what % 2 == 0:\n","            test_pred_1010.iloc[np.where(pca_test_wow.iloc[:,(max_what//2)] >= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n","        else:\n","            test_pred_1010.iloc[np.where(pca_test_wow.iloc[:,(max_what//2)] <= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0   \n","\n","        test_pred_set_1010 = test_pred_set_1010 + test_pred_1010\n","\n","train_pred_set_1010 = train_pred_set_1010/(hhmm- skip_count)\n","val_pred_set_1010 = val_pred_set_1010/(hhmm-skip_count)\n","test_pred_set_1010 = test_pred_set_1010/(hhmm-skip_count)"],"metadata":{"id":"OFPFE1YA5AzO","executionInfo":{"status":"ok","timestamp":1660136732540,"user_tz":-540,"elapsed":107873,"user":{"displayName":"­신기수","userId":"10948147504762254674"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["## 최종 확인\n","# voting해서 50% 이상 1이면 1로, 반대면 0으로\n","final_one_train_df = one_train_df.iloc[np.where(np.round(train_pred_set_1010['Class']) == 1)[0]]\n","final_one_val_df = one_val_df.iloc[np.where(np.round(val_pred_set_1010['Class']) == 1)[0]]\n","final_one_test_df = one_test_df.iloc[np.where(np.round(test_pred_set_1010['Class']) == 1)[0]]\n","\n","# validation score 확인\n","chujung_train2 = pd.DataFrame({'Class':np.repeat(0,len(train_df))})\n","chujung_val2 = pd.DataFrame({'pre_Class':np.repeat(0,len(val_df))})\n","chujung_test2 = pd.DataFrame({'Class':np.repeat(0,len(test_df))})\n","\n","chujung_train2.iloc[final_one_train_df.index,0] = 1\n","chujung_val2.iloc[final_one_val_df.index,0] = 1\n","chujung_test2.iloc[final_one_test_df.index,0] = 1\n","\n","sec_result_train = pd.concat([train_df,chujung_train2], axis=1)\n","sec_result_val = pd.concat([val_df,chujung_val2], axis=1)\n","sec_result_test = pd.concat([test_df,chujung_test2], axis=1)\n","\n","print('F1-score',f1_score(sec_result_val['pre_Class'], val_class, average='macro'))\n","print(confusion_matrix(sec_result_val['pre_Class'], val_class))\n","print(classification_report(sec_result_val['pre_Class'], val_class))\n","\n","# 저장\n","submit = pd.read_csv('sample_submission.csv')\n","submit['Class'] = 0\n","\n","submit.iloc[final_one_test_df.index,1] = 1\n","submit.iloc[final_one_test_df.index,1]\n","\n","submit.to_csv('result_submit.csv', index=False)"],"metadata":{"id":"7t4tiU-QFwnQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660136734144,"user_tz":-540,"elapsed":447,"user":{"displayName":"­신기수","userId":"10948147504762254674"}},"outputId":"f1fdb0bc-9882-43fd-f7af-a7733f9d184e"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["F1-score 0.954501493863888\n","[[28432     5]\n"," [    0    25]]\n","              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     28437\n","           1       0.83      1.00      0.91        25\n","\n","    accuracy                           1.00     28462\n","   macro avg       0.92      1.00      0.95     28462\n","weighted avg       1.00      1.00      1.00     28462\n","\n"]}]}]}